<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>机器学习-Day_14</title>
    <url>/2020/03/20/MLD14/</url>
    <content><![CDATA[<p>吴恩达机器学习第14天</p>
<a id="more"></a>
<h2 id="16-3协同过滤"><a href="#16-3协同过滤" class="headerlink" title="16-3协同过滤"></a>16-3协同过滤</h2><pre><code>collaborative filtering协同过滤</code></pre><p>　　　自行学习特征<br>　　　从用户获得theta值,借线性方程组来求出X特征<br>　　　<img src="/2020/03/20/MLD14/16-3_P1.png" alt="pic"></p>
<h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><p>　　　设已经知道了用户的偏好theta<br>　　　则目标函数为<br>　　　<img src="/2020/03/20/MLD14/16-3_P2.png" alt="pic"></p>
<h3 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h3><p>　　　即先给初始化的theta，在学习的过程中修改用户的偏好。（能不能收敛就不清楚了……）</p>
<h2 id="16-4协同过滤算法"><a href="#16-4协同过滤算法" class="headerlink" title="16-4协同过滤算法"></a>16-4协同过滤算法</h2><p>　　　<img src="/2020/03/20/MLD14/16-4_P1.png" alt="pic"><br>　　　1.初始化x和theta（这里的x是不包括x0的）<br>　　　2.梯度下降<br>　　　3.根据theta和X估计用户的评分</p>
<h2 id="16-5矢量化-低秩矩阵分解"><a href="#16-5矢量化-低秩矩阵分解" class="headerlink" title="16-5矢量化-低秩矩阵分解"></a>16-5矢量化-低秩矩阵分解</h2><pre><code>low rank matrix factorizaton低秩矩阵分解</code></pre><p>　　　<img src="/2020/03/20/MLD14/16-5_P1.png" alt="pic"></p>
<p>　　　机器可以学习到相关的、但是难以用语言描述的特征</p>
<h2 id="16-6实施细节-均值规范化"><a href="#16-6实施细节-均值规范化" class="headerlink" title="16-6实施细节-均值规范化"></a>16-6实施细节-均值规范化</h2><p>　　　设有一个用户没有对电影做出评分，则预测出的偏好都是0，这是不合理的。</p>
<h3 id="均值规范化"><a href="#均值规范化" class="headerlink" title="均值规范化"></a>均值规范化</h3><p>　　　<img src="/2020/03/20/MLD14/16-6_P1.png" alt="pic"><br>　　　其实就是把均值赋给未评分的用户……简单说就是把最受欢迎的推荐给新用户。</p>
<h2 id="17-1学习大数据集"><a href="#17-1学习大数据集" class="headerlink" title="17-1学习大数据集"></a>17-1学习大数据集</h2><p>　　　在进行大数据运算的时候，常常随机选择少量数据进行训练，从学习曲线判断学习效果。</p>
<h2 id="17-2随机梯度下降"><a href="#17-2随机梯度下降" class="headerlink" title="17-2随机梯度下降"></a>17-2随机梯度下降</h2><pre><code>Batch gradient descent批量梯度下降
Stochastic gradient descent随机梯度下降</code></pre><p>　　　1.随机排序这m个样本<br>　　　2.对算法过程反复运算<br>　　　<img src="/2020/03/20/MLD14/17-2_P1.png" alt="pic"><br>　　　按照说法，就是先让theta对样本一拟合一点，再让其和样本二拟合一点，如此下去，即每次只拟合一个样本。对于外层循环，常常是在1-10之间，取决于样本数量大不大。</p>
<h2 id="17-3Mini-Batch下降"><a href="#17-3Mini-Batch下降" class="headerlink" title="17-3Mini-Batch下降"></a>17-3Mini-Batch下降</h2><p>　　　这种梯度下降使用b个样本，常用是[10, 100]<br>　　　当你有一个合适的向量化，那么Mini-Batch会算的更快，因为有高效的代数运算库。</p>
<h2 id="17-4随机梯度下降的收敛"><a href="#17-4随机梯度下降的收敛" class="headerlink" title="17-4随机梯度下降的收敛"></a>17-4随机梯度下降的收敛</h2><p>　　　每内部遍历1000次，就标出代价函数的值。<br>　　　计算代价函数需要在更新theta之前，看看对新样本的代价函数值。<br>　　　<img src="/2020/03/20/MLD14/17-4_P1.png" alt="pic"><br>　　　因为之前是让学习率是个常数，可能会导致在随机梯度下降时在最低点附近徘徊，为了取到更好的最低点，会让学习率随着迭代次数的增加而减少。</p>
<h2 id="17-5在线学习"><a href="#17-5在线学习" class="headerlink" title="17-5在线学习"></a>17-5在线学习</h2><p>　　　从网络访问的数据流中进行在线学习<br>　　　个性化推荐、大数据杀熟-。-</p>
<h2 id="17-6减少映射和数据并行"><a href="#17-6减少映射和数据并行" class="headerlink" title="17-6减少映射和数据并行"></a>17-6减少映射和数据并行</h2><p>　　　Mapreduce<br>　　　在多台机器上进行数据处理（并行运算？）<br>　　　<img src="/2020/03/20/MLD14/17-4_P1.png" alt="pic"></p>
<h2 id="18-1问题描述和OCR流水线"><a href="#18-1问题描述和OCR流水线" class="headerlink" title="18-1问题描述和OCR流水线"></a>18-1问题描述和OCR流水线</h2><p>　　　1.文本识别<br>　　　2.单词分离<br>　　　3.单词判断<br>　　　4.单词修正</p>
<h2 id="18-2滑动窗口"><a href="#18-2滑动窗口" class="headerlink" title="18-2滑动窗口"></a>18-2滑动窗口</h2><p>　　　行人检测</p>
]]></content>
      <tags>
        <tag>/machineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-Day_12</title>
    <url>/2020/03/19/MLD12/</url>
    <content><![CDATA[<p>吴恩达机器学习第12天</p>
<a id="more"></a>
<h2 id="15-1问题动机"><a href="#15-1问题动机" class="headerlink" title="15-1问题动机"></a>15-1问题动机</h2><pre><code>异常检测anomaly detection</code></pre><h3 id="密度估计"><a href="#密度估计" class="headerlink" title="密度估计"></a>密度估计</h3><p>　　　<img src="/2020/03/19/MLD12/15-1_P1.png" alt="pic"><br>　　　感觉有点像高斯分布的3sigma法则……</p>
<p>　　　建一个模型p(X)<br>　　　若p(x)小于某个阈值则判断为异常</p>
<h2 id="15-2高斯分布"><a href="#15-2高斯分布" class="headerlink" title="15-2高斯分布"></a>15-2高斯分布</h2><h2 id="15-3算法"><a href="#15-3算法" class="headerlink" title="15-3算法"></a>15-3算法</h2><p>　　　假设各变量之间独立，所以有<br>　　　$$P(X) = P(X_1;\mu,\sigma_1^2)P(X_2;\mu,\sigma_2^2) \dots P(X_n;\mu,\sigma_n^2) = \prod_{j=1}^n{P(X_j;\mu_j,\sigma^2_j)}$$</p>
<p>　　　即使不独立用这个代码也可以算出较好的答案</p>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>　　　1.选择合适的特征$x_i$<br>　　　2.参数拟合$\mu,\sigma^2$<br>　　　3.给一个新的样本x，计算p(x)<br>　　　4.判断是否异常（小于阈值）<br>　　　<img src="/2020/03/19/MLD12/15-3_P1.png" alt="pic"></p>
<h2 id="15-4开发和评估异常检测系统"><a href="#15-4开发和评估异常检测系统" class="headerlink" title="15-4开发和评估异常检测系统"></a>15-4开发和评估异常检测系统</h2><p>　　　开发一个评估系统，决定是否纳入一个新的特征。<br>　　　1. 假设你有一些标签样本，标明哪个是异常哪个是正常<br>　　　2. 正常的训练集<br>　　　3. 交叉集和测试集<br>　　　假设你有10000个正常数据和20个异常数据，分配如下<br>　　　<img src="/2020/03/19/MLD12/15-4_P1.png" alt="pic">
　　　</p>
<h3 id="算法评估"><a href="#算法评估" class="headerlink" title="算法评估"></a>算法评估</h3><p>　　　评估标准：<br>　　　　　　-真假阴阳性<br>　　　　　　-召回率和命中率<br>　　　　　　-F1-score<br>　　　还有p(x)的阈值epslion</p>
<h2 id="15-5异常检测VS监督学习"><a href="#15-5异常检测VS监督学习" class="headerlink" title="15-5异常检测VS监督学习"></a>15-5异常检测VS监督学习</h2><p>$$<br>\begin{array}{l|l}<br>{异常检测}&amp;{监督学习}\\<br>\hline<br>{非常少的正样本，常常是0-20个}&amp;{很大数量的正样本和负样本}\\<br>{大量的负样本}\\<br>{如果有多种类的异常，很难从正样本看出是哪种异常}&amp;{}\\<br>{很难预测出未在正样本中出现的异常}&amp;{有足够的正样本去预测是何种异常和未知异常}\\<br>\end{array}<br>$$<br>　　　所以如果使用异常检测的时候需要用大量的负样本来保证算法能够学到足够多的知识<br>　　　<strong>从应用上</strong><br>$$<br>\begin{array}{l|l}<br>{异常检测}&amp;{监督学习}\\<br>\hline<br>{欺骗检测}&amp;{邮件分类}\\<br>{工业生产}&amp;{天气预测}\\<br>{数据监测机器}&amp;{癌症分类}\\<br>\end{array}<br>$$<br>　　　如果异常样本较少，则使用异常检测，如果正负样本都很多，则用监督学习。</p>
<h2 id="15-6选择要使用的功能"><a href="#15-6选择要使用的功能" class="headerlink" title="15-6选择要使用的功能"></a>15-6选择要使用的功能</h2><p>　　　非高斯特征尝试转换为高斯特征。<br>　　　取对数变换、开根号变换<br>　　　<img src="/2020/03/19/MLD12/15-6_P1.png" alt="pic"><br>　　　话说取什么函数要一个一个试吗=.=</p>
<h3 id="异常检测的误差分析"><a href="#异常检测的误差分析" class="headerlink" title="异常检测的误差分析"></a>异常检测的误差分析</h3><p>　　　尝试创建一个新特征，给异常样本较低的概率<br>　　　<img src="/2020/03/19/MLD12/15-6_P2.png" alt="pic"><br>　　　通过特征间关系组合出一个新的特征，来更好的监测某个错误。</p>
<h2 id="15-7多变量高斯分布"><a href="#15-7多变量高斯分布" class="headerlink" title="15-7多变量高斯分布"></a>15-7多变量高斯分布</h2><p>　　　<img src="/2020/03/19/MLD12/15-7_P1.png" alt="pic"><br>　　　为了解决某些不符合常规数据，但是却在单个特征的高斯分布中占大概率的情况。<br>　　　<img src="/2020/03/19/MLD12/15-7_P2.png" alt="pic"></p>
<h3 id="协方差矩阵的影响"><a href="#协方差矩阵的影响" class="headerlink" title="协方差矩阵的影响"></a>协方差矩阵的影响</h3><p>　　　以下是对2*2的协方差矩阵而言<br>　　　左上角元素决定在x1方向上的缩放率<br>　　　右下角元素决定在x2方向上的缩放率<br>　　　<img src="/2020/03/19/MLD12/15-7_P3.png" alt="pic"><br>　　　<img src="/2020/03/19/MLD12/15-7_P4.png" alt="pic"><br>　　　如果是负数的话代表等高线向另一个方向倾斜</p>
<h3 id="μ的影响"><a href="#μ的影响" class="headerlink" title="μ的影响"></a>μ的影响</h3><p>　　　<img src="/2020/03/19/MLD12/15-7_P5.png" alt="pic"><br>　　　改变分布的中心</p>
<h2 id="15-8使用多变量高斯分布的异常检测"><a href="#15-8使用多变量高斯分布的异常检测" class="headerlink" title="15-8使用多变量高斯分布的异常检测"></a>15-8使用多变量高斯分布的异常检测</h2><p>　　　参数估计后再带入公式<br>　　　<img src="/2020/03/19/MLD12/15-8_P1.png" alt="pic"><br>　　　<br>　　　传统的模型，即原来用概率累积的方式，是轴对齐的；原始模型的计算成本比较低；适用于创造不同特征之间组合产生的新的特征；即使样本集数量少也能够使用；</p>
<p>　　　新的模型会自动捕捉特征之间的相关性；计算代价大；且必须保证样本集大于特征否则协方差矩阵不可逆。(m ≥ 10n)；协方差矩阵不可逆的原因可能是样本数量太少，或者冗余（即线性相关）的特征太多。</p>
<h2 id="16-1推荐系统的问题规划"><a href="#16-1推荐系统的问题规划" class="headerlink" title="16-1推荐系统的问题规划"></a>16-1推荐系统的问题规划</h2><p>　　　参数介绍<br>　　　$n_u$表示用户号，$n_m$表示电影标号<br>　　　$r(i,j)=1$ 表示用户j已经对电影i评过分<br>　　　$y^{(i,j)}$ 当用户j对电影i评过分时，评分的高低</p>
<p>　　　<img src="/2020/03/19/MLD12/16-1_P1.png" alt="pic"></p>
<h2 id="16-2基于内容的推荐算法"><a href="#16-2基于内容的推荐算法" class="headerlink" title="16-2基于内容的推荐算法"></a>16-2基于内容的推荐算法</h2><p>　　　根据特征，对内容进行分类。<br>　　　求出某个参数theta，根据求出的theta乘上x来预测。</p>
<p>　　　优化目标<br>　　　　　　<img src="/2020/03/19/MLD12/16-2_P1.png" alt="pic"></p>
<p>　　　梯度下降<br>　　　　　　<img src="/2020/03/19/MLD12/16-2_P2.png" alt="pic"></p>
]]></content>
      <tags>
        <tag>/machineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-day_11</title>
    <url>/2020/03/19/MLD11/</url>
    <content><![CDATA[<p>吴恩达机器学习Day11</p>
<a id="more"></a>
<h2 id="13-3优化目标"><a href="#13-3优化目标" class="headerlink" title="13-3优化目标"></a>13-3优化目标</h2><p>　　　$c^{(i)}$表示样本$x^{(i)}$所属的聚类中心index<br>　　　$\mu_k$第k个均值中心的位置<br>　　　$\mu_c^{(i)}$表示$x^{(i)}$所属的聚类中心的位置<br>　　　如$x^{(i)}=5, c^{(i)}=5, so \; \mu_c^{(i)}=\mu_5 $<br>　　　优化目标函数，也叫失真代价函数<em>(distortion cost function)</em><br>　　　<img src="/2020/03/19/MLD11/13-3_P1.png" alt="pic"><br>　　　K-均值算法<br>　　　初始化聚类中心的值<br>　　　弹幕提到这和物理上的旋转类似，旋转的最后结果总是会围绕某个轴。<br>　　　<img src="/2020/03/19/MLD11/13-3_P2.png" alt="pic"><br>　　　找出每个点所属的聚类中心的值也相当于最小化代价函数<br>　　　其次是根据均值找出各聚类中心的新位置</p>
<h2 id="13-4随机初始化"><a href="#13-4随机初始化" class="headerlink" title="13-4随机初始化"></a>13-4随机初始化</h2><p>　　　1. 聚类中心的数量必须小于样本量<br>　　　2. 随机选取K个训练样本<br>　　　3. 将这些训练样本作为聚类中心</p>
<p>　　　局部最优<br>　　　　　　<img src="/2020/03/19/MLD11/13-4_P1.png" alt="pic"><br>　　　做法<br>　　　　　　多次随机化聚类中心，运行算法，计算代价函数。</p>
<h2 id="13-5选取聚类数量"><a href="#13-5选取聚类数量" class="headerlink" title="13-5选取聚类数量"></a>13-5选取聚类数量</h2><pre><code>肘部法则 the Elbow Method</code></pre><p>　　　改变K的数量，计算各种情况的代价函数值<br>　　　<img src="/2020/03/19/MLD11/13-4_P1.png" alt="pic"><br>　　　如果和左图所示，图中有明显拐点，则拐点是较佳的K值，但如果右图没有明显拐点的就很难选出适合的K值</p>
<p>　　　查找什么K值适合后续目的<br>　　　如服装，需要分成大中小三种码数，则此时选择3种聚类中心是合适的。</p>
<h2 id="14-1目标1——数据压缩"><a href="#14-1目标1——数据压缩" class="headerlink" title="14-1目标1——数据压缩"></a>14-1目标1——数据压缩</h2><p>　　　维度压缩；减少类似的、无关紧要的特征。<br>　　　举例：将二维特征压缩为一维特征<br>　　　　　　<img src="/2020/03/19/MLD11/14-1_P1.png" alt="pic"></p>
<p>　　　将三维特征压缩为二维特征<br>　　　　　　将三维空间点投影到一个二维平面上<br>　　　　　　<img src="/2020/03/19/MLD11/14-1_P2.png" alt="pic"></p>
<h2 id="14-2目标2——可视化"><a href="#14-2目标2——可视化" class="headerlink" title="14-2目标2——可视化"></a>14-2目标2——可视化</h2><p>　　　将大量的特征压缩为2-3个具有特征性的特征，然后画在图表上查看</p>
<h2 id="14-3主成分分析问题规划1"><a href="#14-3主成分分析问题规划1" class="headerlink" title="14-3主成分分析问题规划1"></a>14-3主成分分析问题规划1</h2><pre><code>Principal Component Analysis(PCA)</code></pre><p>　　　PCA找到一个投影平面，在投影误差最小的前提下，将高维数据压缩为低维。</p>
<p>　　　将n维数据压缩到k维：找到k个向量$u^{(1)} \dots u^{(k)}$来投影这些样本点，最小化这个投影误差。</p>
<p>　　　尽管看上有些像素，但是PCA和线性回归不同，在PCA中计算的是投影误差（正交误差），而线性回归是垂直误差。</p>
<h2 id="14-4主成分分析问题规划2"><a href="#14-4主成分分析问题规划2" class="headerlink" title="14-4主成分分析问题规划2"></a>14-4主成分分析问题规划2</h2><p>　　　1. 预处理：特征归一化和均值标准化<br>　　　2. 计算协方差<em>covariance matrix</em><br>　　　$$\Sigma = \frac{1}{m}\sum_{i=1}^{n}{(x^{(i)}-\mu)(x^{(i)}-\mu)^T}$$<br>　　　(貌似图片给的公式有误)<br>　　　3. 计算协方差矩阵的特征向量<br>　　　$$[U,S,V] = svd(Sigma)$$<br>　　　<img src="/2020/03/19/MLD11/14-4_P1.png" alt="pic"><br>　　　U的前k个向量既是投影方向<br>　　　4. 用算出来的投影方向乘上X得到新的向量Z<br>　　　<img src="/2020/03/19/MLD11/14-4_P2.png" alt="pic"></p>
<h2 id="14-5主成分数量选择"><a href="#14-5主成分数量选择" class="headerlink" title="14-5主成分数量选择"></a>14-5主成分数量选择</h2><p>　　　选择k：1. 平均平方投影误差<br>　　　　　　 2. 计算数据的总方差<br>　　　　　　 3. 选择最小的k使得下面是自≤0.01，即“99%的方差性得以保留”<br>　　　<img src="/2020/03/19/MLD11/14-5_P1.png" alt="pic"></p>
<h3 id="选择k的算法"><a href="#选择k的算法" class="headerlink" title="选择k的算法"></a>选择k的算法</h3><p>　　　<img src="/2020/03/19/MLD11/14-5_P2.png" alt="pic"><br>　　　这个S是一个n*n的矩阵，除了对角线之外其他元素都是0，可是不知道为什么用$$1-\frac{\sum_{i=1}^k{S_{ii}}}{\sum_{i=1}^n{S_{ii}}} \leq 0.01$$</p>
<h2 id="14-6压缩重现"><a href="#14-6压缩重现" class="headerlink" title="14-6压缩重现"></a>14-6压缩重现</h2><p>　　　<img src="/2020/03/19/MLD11/14-6_P1.png" alt="pic"><br>　　　$$X_approx^{(1)} = U_reduce · z^{(1)}$$</p>
<h2 id="14-7应用PCA的建议"><a href="#14-7应用PCA的建议" class="headerlink" title="14-7应用PCA的建议"></a>14-7应用PCA的建议</h2><h3 id="加速监督学习"><a href="#加速监督学习" class="headerlink" title="加速监督学习"></a>加速监督学习</h3><p>　　　<img src="/2020/03/19/MLD11/14-7_P1.png" alt="pic"></p>
<h3 id="错误使用PCA"><a href="#错误使用PCA" class="headerlink" title="错误使用PCA"></a>错误使用PCA</h3><p>　　　PCA不是防止过拟合的方式。PCA在不考虑y值的情况下舍弃掉了一些有用的信息，只是单纯保持了样本的方差。</p>
<p>　　　PCA并非一定要用，在应用PCA之前先使用原始数据，如果不能很好地拟合的话才会使用PCA算法。</p>
]]></content>
      <tags>
        <tag>/machineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>套接字程序设计</title>
    <url>/2020/03/16/Socket-Programming/</url>
    <content><![CDATA[<p>套接字程序设计</p>
<h2 id="什么是套接字"><a href="#什么是套接字" class="headerlink" title="什么是套接字"></a>什么是套接字</h2><p>　　　<img src="/2020/03/16/Socket-Programming/socket_P1.png" alt="pic"><br>　　　步骤：1.通过IP地址或者域名找到主机[如 <a href="http://www.xxx.com" target="_blank" rel="noopener">www.xxx.com</a> -&gt; 202.116.64.8]<br>　　　　　　2.通过端口号找到主机上的进程(UDP协议[不可靠]，TCP协议[面向连接、可靠])</p>
<h3 id="端口号"><a href="#端口号" class="headerlink" title="端口号"></a>端口号</h3><p>　　　端口号由16bit组成</p>
<a id="more"></a>
<p>　　　其中包括</p>
<img src="/2020/03/16/Socket-Programming/socket_P2.png" class="[pic]">]]></content>
      <tags>
        <tag>/ComputerNetwork</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-Day_10</title>
    <url>/2020/03/15/MLD10/</url>
    <content><![CDATA[<p>吴恩达机器学习-Day_10</p>
<a id="more"></a>

<h2 id="12-3大间距分类器的数学原理"><a href="#12-3大间距分类器的数学原理" class="headerlink" title="12-3大间距分类器的数学原理"></a>12-3大间距分类器的数学原理</h2><p>　　　向量的内积<br>　　　范数<em>norm</em>(||$\vec m$||):向量的长度<br>　　　<img src="/2020/03/15/MLD10/12-3_P1.png" alt="pic"><br>　　　即两个向量的内积等于u的范数乘上p在向量方向上的投影<br>　　　<img src="/2020/03/15/MLD10/12-3_P2.png" alt="pic"><br>　　　类似上面的内容将$\theta^TX化成p · ||\theta||$<br>　　　<img src="/2020/03/15/MLD10/12-3_P3.png" alt="pic"><br>　　　目标函数是最小化theta，为什么这个公式是这样和以前不同，我觉得是因为支持向量机通过令X*theta大于1和小于-1的方法令前面那部分变成了0，所以只需要考虑后面那部分系数的影响。<br>　　　而为了使theta足够小的同时满足大于1和小于-1的条件，投影p需要足够的大，投影映射到图像上便是所谓的间距，这就是为什么支持向量机能够有大间距的性质。</p>
<h2 id="12-4核函数"><a href="#12-4核函数" class="headerlink" title="12-4核函数"></a>12-4核函数</h2><p>　　　核函数<em>kernel function</em><br>　　　<img src="/2020/03/15/MLD10/12-4_P1.png" alt="pic"><br>　　　高斯核函数<br>　　　<img src="/2020/03/15/MLD10/12-4_P2.png" alt="pic"><br>　　<br>　　　核函数和相似函数<br>　　　$$f_1 = similarity\left(x,l^{\left(1\right)}\right)=exp\left(-\frac{\sum_{j=1}^n{\left(x_j-l_j^{\left(1\right)}\right)^2}}{2\sigma^2}\right)$$</p>
<p>　　　如果x约等于$l^{\left(1\right)}$, 则f1会约等于1<br>　　　如果x和$l^{\left(1\right)}$相差很远，那么f1会约等于0</p>
<p>　　　<img src="/2020/03/15/MLD10/12-4_P3.png" alt="pic"><br>　　　sigma对函数图像的影响</p>
<p>　　　<img src="/2020/03/15/MLD10/12-4_P4.png" alt="pic"><br>　　　核函数的做法即为，选定几个标记点landmark，则在某些标记点预测值为1的情况下，接近这些标记点的样本预测值便为1，远离则为0.</p>
<h2 id="12-5核函数-补充"><a href="#12-5核函数-补充" class="headerlink" title="12-5核函数-补充"></a>12-5核函数-补充</h2><p>　　　<img src="/2020/03/15/MLD10/12-5_P1.png" alt="pic"><br>　　　标记点的选取，即把所有的样本点都当成标记点去计算</p>
<p>　　　<img src="/2020/03/15/MLD10/12-5_P2.png" alt="pic"><br>　　　这边讲解了训练的函数，在某些应用中，在进行正则化的时候会使用$\theta^TM\theta$,这里的M是一个特殊的矩阵，使用这个矩阵来正则化能够加快运算的速度</p>
<p>　　　在SVM的参数中，有C和sigma，两个参数对函数图像的影响如下<br>　　　<img src="/2020/03/15/MLD10/12-5_P3.png" alt="pic"></p>
<h2 id="12-6使用SVM"><a href="#12-6使用SVM" class="headerlink" title="12-6使用SVM"></a>12-6使用SVM</h2><p>　　　举例：线性核函数、高斯核函数、多项式核函数、字符串核函数、卡方核函数、直方相交核函数……<br>　　　在使用函数前进行特征归一化<br>　　　使用的核函数需要满足默塞尔定理</p>
<p>　　　根据样本数和特征数进行选择<br>　　　<img src="/2020/03/15/MLD10/12-6_P1.png" alt="pic"><br>　　　即当特征数小，样本数量适中时，才是用高斯核函数<br>　　　其他情况下，如特征数大，样本数量过小，或者特征数小，样本数量大的时候，通常去使用线性核函数或者线性回归</p>
<h2 id="13-1无监督学习"><a href="#13-1无监督学习" class="headerlink" title="13-1无监督学习"></a>13-1无监督学习</h2><pre><code>无监督学习*unsupervised learning*: 样本点不提供标签
簇分配过程*cluster assignment step*</code></pre><h2 id="13-2K-means算法"><a href="#13-2K-means算法" class="headerlink" title="13-2K-means算法"></a>13-2K-means算法</h2><pre><code>聚类中心 cluster centroids</code></pre><p>　　　过程：遍历所有的样本，根据与聚类中心的距离将所有的点分成两类。将两类的点计算均值，再将聚类中心移动到均值处的位置。<br>　　　算法过程如下:<br>　　　<img src="/2020/03/15/MLD10/13-2_P1.png" alt="pic"><br>　　　如果遇到没有点的聚类中心，正常是将其移除，如果需要K的簇，则再重新随机化一个值。</p>
]]></content>
      <tags>
        <tag>/machineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>KMP算法</title>
    <url>/2020/03/11/KMP%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[
<div style="position:relative; width: 100%; height: 0; padding-bottom: 75%"><iframe src="//player.bilibili.com/player.html?aid=49930100&cid=119839716&page=1" scrolling="yes" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe></div>
]]></content>
      <tags>
        <tag>/algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习Day_9</title>
    <url>/2020/03/09/MLD9/</url>
    <content><![CDATA[<p>吴恩达机器学习day_9</p>
<a id="more"></a>
<h2 id="10-2评估假设"><a href="#10-2评估假设" class="headerlink" title="10-2评估假设"></a>10-2评估假设</h2><p>　　　随机分布后按照7：3把数据集分别分成训练集和测试集<br>　　　在训练集中计算参数theta<br>　　　计算测试集误差<br>　　　得到非分类误差<em>Misclassfication error</em>,即测试集代价函数值大于阈值，可是最后实际结果却归为1.</p>
<h2 id="10-3模型选择和训练、验证测试集"><a href="#10-3模型选择和训练、验证测试集" class="headerlink" title="10-3模型选择和训练、验证测试集"></a>10-3模型选择和训练、验证测试集</h2><h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>　　　设置参数d表示级数，d=1表示最高次数为1.<br>　　　通过每一个不同的d学习出不同的theta，再计算该级数下算出的theta的代价函数值。但是拟合后的最优模型可能是对测试集表现较好，但是对新样本表现可能不一定很好。<br>　　　 解决方法<br>　　　　　　-将样本分为6：2：2，分别为训练集、交叉偏差集cv和测试集<br>　　　<img src="/2020/03/09/MLD9/10-3_P1.png" alt="pic"><br>　　　和之前不同的是，这边用的是交叉测试集去测试参数d选出合适的次数，再用这个d去你和参数。</p>
<p>　　　使用测试集选出模型，再用同一个测试集拟合参数不是一个好主意。</p>
<h2 id="10-4诊断偏差和方差"><a href="#10-4诊断偏差和方差" class="headerlink" title="10-4诊断偏差和方差"></a>10-4诊断偏差和方差</h2><p>　　　<img src="/2020/03/09/MLD9/10-4_P1.png" alt="pic"><br>　　　<img src="/2020/03/09/MLD9/10-4_P2.png" alt="pic"><br>　　　　　　如果训练误差和交叉误差都很大，说明是高偏差的欠拟合<br>　　　　　　如果训练误差小而交叉误差大，说明是高方差的过拟合</p>
<h2 id="10-5正则化的偏差和误差"><a href="#10-5正则化的偏差和误差" class="headerlink" title="10-5正则化的偏差和误差"></a>10-5正则化的偏差和误差</h2><p>　　　找出合适的lambda值<br>　　　　　　这里使用的是翻倍去计算<br>　　　<img src="/2020/03/09/MLD9/10-5_P1.png" alt="pic"><br>　　　<img src="/2020/03/09/MLD9/10-5_P2.png" alt="pic"></p>
<h2 id="10-6学习曲线"><a href="#10-6学习曲线" class="headerlink" title="10-6学习曲线"></a>10-6学习曲线</h2><h3 id="正常情况"><a href="#正常情况" class="headerlink" title="正常情况"></a>正常情况</h3><p>　　　<img src="/2020/03/09/MLD9/10-6_P1.png" alt="pic"><br>　　　　　　这里的误差是平均误差, 且训练误差最后接近训练集的误差</p>
<h3 id="高偏差"><a href="#高偏差" class="headerlink" title="高偏差"></a>高偏差</h3><p>　　　<img src="/2020/03/09/MLD9/10-6_P2.png" alt="pic"><br>　　　　　　训练误差最后接近交叉集的误差，不过误差都较高，且更快趋于平缓.即在此之后不管如何增加训练集规模效果都不会增加</p>
<h3 id="高方差"><a href="#高方差" class="headerlink" title="高方差"></a>高方差</h3><p>　　　<img src="/2020/03/09/MLD9/10-6_P3.png" alt="pic"><br>　　　　　　增加更多数据到训练集可以提高算法效果</p>
<h2 id="10-7决定接下来做什么"><a href="#10-7决定接下来做什么" class="headerlink" title="10-7决定接下来做什么"></a>10-7决定接下来做什么</h2><p>　　　- 获得更多的训练集  ——解决高方差<br>　　　- 尝试更小的特征  ——解决高方差<br>　　　- 尝试增加额外的特征 ——解决高误差<br>　　　- 尝试增加高级幂的特征 ——解决高误差<br>　　　- 尝试减少正则化系数lambda ——解决高误差<br>　　　- 尝试增加正则化系数lambda ——解决高方差<br>　　　<img src="/2020/03/09/MLD9/10-7_P1.png" alt="pic"></p>
<h2 id="11-1决定执行的优先级"><a href="#11-1决定执行的优先级" class="headerlink" title="11-1决定执行的优先级"></a>11-1决定执行的优先级</h2><p>　　　例如在邮件分类器里。<br>　　　这里模型的特征是关键词，出现关键词则为1，否则为0.<br>　　　如何减少误差<br>　　　　　　- 收集更多的数据<br>　　　　　　- 基于邮件来源（邮件标题）使用更加复杂的特征　　　　　　- 基于邮件信息（邮件主体）使用更加复杂的特征（如discount和discounts）<br>　　　　　　- 基于识别错拼字使用更加复杂的算法</p>
<h2 id="11-2误差分析"><a href="#11-2误差分析" class="headerlink" title="11-2误差分析"></a>11-2误差分析</h2><p>　　　- 从一个简单实现的算法开始，在验证集上应用并测试。<br>　　　- 画出学习曲线决定是否需要更多的数据和特征<br>　　　- 误差分析: 人为测试被错误分类的误差，从而寻找能否选出更加系统的特征或更好的修正方法。</p>
<h3 id="数值计算的重要性"><a href="#数值计算的重要性" class="headerlink" title="数值计算的重要性"></a>数值计算的重要性</h3><p>　　　如词干提取<em>stemming</em>, 邮件分类是否需要词干提取，取决于使用这种方法能否有效提高分类正确率。<br>　　　从数学的角度上，最好使用验证集来进行误差分析。</p>
<h2 id="11-3不对称性分类的误差评估"><a href="#11-3不对称性分类的误差评估" class="headerlink" title="11-3不对称性分类的误差评估"></a>11-3不对称性分类的误差评估</h2><p>　　　偏斜类<em>skewed classes</em>: 正和负的比例处于极端的情况<br>　　　对偏斜类进行误差分析无法评价算法效能</p>
<p>　　　查准率<em>Precision</em>: 在预测值为1的情况下有多少真正值为1.<br>　　　召回率<em>Recall</em>: 在真正值为1的情况下有多少预测值为1<br>　　　<img src="/2020/03/09/MLD9/11-3_P1.png" alt="pic"></p>
<h2 id="11-4查准率和召回率的权衡"><a href="#11-4查准率和召回率的权衡" class="headerlink" title="11-4查准率和召回率的权衡"></a>11-4查准率和召回率的权衡</h2><p>　　　修改阈值，如果阈值提高，则会增加查准率，但是会降低召回率。反之，则会减少查准率，提高召回率。</p>
<h3 id="如何从查准率和召回率抉择不同算法"><a href="#如何从查准率和召回率抉择不同算法" class="headerlink" title="如何从查准率和召回率抉择不同算法"></a>如何从查准率和召回率抉择不同算法</h3><p>　　　设P为查准率，R为召回率，用算数平均值$\frac{P+R}{2}$并不是一个很好的方法<br>　　　$F_1Score:2\frac{PR}{P+R}$<br>　　　用F值可以避免选到P或者R取到极端小值的情况。</p>
<h2 id="11-5机器学习数据"><a href="#11-5机器学习数据" class="headerlink" title="11-5机器学习数据"></a>11-5机器学习数据</h2><p>　　　- 大量的数据量才能体现出好的算法的优势<br>　　　- 合适的特征能够帮助更好的预测<br>　　　- 使用多个特征减少偏差，训练集数量大于特征量能够有效解决高方差带来的过拟合。</p>
<h2 id="12-1优化目标"><a href="#12-1优化目标" class="headerlink" title="12-1优化目标"></a>12-1优化目标</h2><p>　　　支持向量机<em>the Support Vector Machine, SVM</em>.<br>　　　<img src="/2020/03/09/MLD9/12-1_P1.png" alt="pic"><br>　　　用两条线来替代逻辑回归的函数图像<br>　　　<img src="/2020/03/09/MLD9/12-1_P2.png" alt="pic"><br>　　　支持向量机公式的来源<br>　　　如果X*theta&gt;=0， 则预测为1，否则预测为0</p>
<h2 id="12-2直观上对大间隔的理解"><a href="#12-2直观上对大间隔的理解" class="headerlink" title="12-2直观上对大间隔的理解"></a>12-2直观上对大间隔的理解</h2><p>　　　<img src="/2020/03/09/MLD9/12-2_P1.png" alt="pic"></p>
<h3 id="SVM的决策边界"><a href="#SVM的决策边界" class="headerlink" title="SVM的决策边界"></a>SVM的决策边界</h3><p>　　　<img src="/2020/03/09/MLD9/12-2_P2.png" alt="pic"><br>　　　<img src="/2020/03/09/MLD9/12-2_P3.png" alt="pic"><br>　　　例如线性可分，如果假设C是一个很大的数时，支持向量机会选择距离两种样本集间距<em>margin</em>最大的决策边界。<br>　　　<img src="/2020/03/09/MLD9/12-2_P3.png" alt="pic"><br>　　　上面提到C足够大的话，会按照两个样本集之间的最大间距去区分，但是如果出现了异常数据的话会过于敏感，像上图由于出现了一场样本，原本较为正确的黑线被调整为粉线。所以C要取到适合的值。</p>
]]></content>
      <tags>
        <tag>/machineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习_Day7</title>
    <url>/2020/03/06/MLD7/</url>
    <content><![CDATA[<p>吴恩达机器学习Day7</p>
<a id="more"></a>
<h2 id="8-4模型展示"><a href="#8-4模型展示" class="headerlink" title="8-4模型展示"></a>8-4模型展示</h2><p>　　　前向传播<em>Forward propagation</em>的神经网络。方程的向量化表示。<br>　　　<img src="/2020/03/06/MLD7/8-4_P1.png" alt="pic"><br>　　　注意这里的$z^{\left(2\right)}=\Theta^{\left(1\right)}a^{\left(1\right)}$ 是把x也看成了a，然后在计算之后需要给各个层加偏置项，即下标为0的那一项。</p>
<p>　　　<img src="/2020/03/06/MLD7/8-4_P2.png" alt="pic"><br>　　　这一步是从隐藏层推出输出层，这一步和逻辑回归的假设函数相似，这不过区别在于标记Theta，且输入的特征是来自于隐藏层，而不是原本数据的特征。<br>　　　即没有使用样本的特征，而是用训练出来的逻辑回归的结果作为特征。</p>
<p>　　　其它的神经网络的结构，不止上面的三层结构，可能有多个隐藏层。</p>
<h2 id="8-5例子和直觉理解"><a href="#8-5例子和直觉理解" class="headerlink" title="8-5例子和直觉理解"></a>8-5例子和直觉理解</h2><p>　　　<img src="/2020/03/06/MLD7/8-5_P1.png" alt="pic"><br>　　　神经网络运用在与门上，加入一个偏置单元。Theta的值是由教授本身提供。g(4.6)的值接近0.99<br>　　　这些例子只有两层，还是算比较入门的神经网络</p>
<h2 id="8-6例子和直觉理解"><a href="#8-6例子和直觉理解" class="headerlink" title="8-6例子和直觉理解"></a>8-6例子和直觉理解</h2><p>　　　<img src="/2020/03/06/MLD7/8-6_P1.png" alt="pic"><br>　　　神经网络算出非门的假设函数</p>
<p>　　　加下来是比较复杂的<br>　　　<img src="/2020/03/06/MLD7/8-6_P2.png" alt="pic"><br>　　　这一步相当于 $(x_1  AND  x_2) OR (NOT  x_1  AND  NOT  x_2) = x_1  XNOR  x_2$<br>　　　有点像逻辑的相互叠加。</p>
<h2 id="8-7多元分类"><a href="#8-7多元分类" class="headerlink" title="8-7多元分类"></a>8-7多元分类</h2><p>　　　<img src="/2020/03/06/MLD7/8-6_P1.png" alt="pic"></p>
<h2 id="9-1代价函数"><a href="#9-1代价函数" class="headerlink" title="9-1代价函数"></a>9-1代价函数</h2><p>　　　<img src="/2020/03/06/MLD7/9-1_P1.png" alt="pic"><br>　　　L: 在神经网络里层次数量<br>　　　$s_l$: 在层次l里有几个单元<br>　　　<img src="/2020/03/06/MLD7/9-1_P2.png" alt="pic"><br>　　　神经网络的代价函数: K是指最后y有几个输入，如[0 0 1 0]`这种的就有4种输出，至于在正规化项中的求和，$s_l+1$是加上偏置项+1的结果；因为每个单元之间相连，所以有$s_l+1*s_l$种链接方式，除去输出层后，总共是L-1层。</p>
<h2 id="9-2反向传播算法"><a href="#9-2反向传播算法" class="headerlink" title="9-2反向传播算法"></a>9-2反向传播算法</h2><p>　　　反向传播算法<em>Back propagation</em><br>　　　<img src="/2020/03/06/MLD7/9-2_P1.png" alt="pic"><br>　　　<img src="/2020/03/06/MLD7/9-2_P2.png" alt="pic"><br>　　　这边的梯度我也没看懂怎么推的，省略太多步了，后面有时间再来看看吧。<br>　　　<img src="/2020/03/06/MLD7/9-2_P3.png" alt="pic"><br>　　　先用正向传播来求出每一层的激活项a，然后你用反向传播求出每一层的$\delta$，$\Delta$累加上激活项和$\delta$的乘积，加上正则化项之后才是正规化的梯度下降D。这一节的公式挺好理解，但是原理和数学推导看着一脸懵。</p>
<h2 id="9-3理解反向传播"><a href="#9-3理解反向传播" class="headerlink" title="9-3理解反向传播"></a>9-3理解反向传播</h2><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>　　　<img src="/2020/03/06/MLD7/9-3_P1.png" alt="pic"><br>　　　这个前向传播和前面的与或门类似。</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>　　　<img src="/2020/03/06/MLD7/9-3_P2.png" alt="pic"><br>　　　这里忽略了正则化项，且把代价函数近似的看作(h(x)-y)的平方。<br>　　　<img src="/2020/03/06/MLD7/9-3_P3.png" alt="pic"><br>　　　怎么感觉有点像递归啊……前面讲的delta计算公式和这边写的不一样，应该是因为前面的激活项乘上系数theta之后变换变成了这一边的delta。</p>
<h2 id="9-4使用注意：展开参数"><a href="#9-4使用注意：展开参数" class="headerlink" title="9-4使用注意：展开参数"></a>9-4使用注意：展开参数</h2><p>　　　如何把参数从矩阵变成向量<br>　　　<img src="/2020/03/06/MLD7/9-4_P1.png" alt="pic"><br>　　　这里我觉得只有三层，应该是只有2个theta……，所以我觉得应该是没有存在theta2，或者theta1.<br>　　　将向量化后的矩阵输入到代价函数中，然后再在函数体内reshape成矩阵。</p>
<h2 id="9-5梯度检测"><a href="#9-5梯度检测" class="headerlink" title="9-5梯度检测"></a>9-5梯度检测</h2><p>　　　<em>Gradient Checking</em>，在反向传播中可能会出现bug产生误差，需要梯度检测，以下是原理和实现方式。<br>　　　<img src="/2020/03/06/MLD7/9-5_P1.png" alt="pic"><br>　　　<img src="/2020/03/06/MLD7/9-5_P2.png" alt="pic"><br>　　　实现的代码<br>　　　<img src="/2020/03/06/MLD7/9-5_P3.png" alt="pic"></p>
<p>　　　整个过程步骤如下：<br>　　　　　　用反向传播计算DVec<br>　　　　　　使用梯度下降检测<br>　　　　　　确定两者的值相似<br>　　　　　　关闭梯度检测，使用反向传播的代码去学习，因为梯度检测的代码太慢</p>
<h2 id="9-6随机初始化"><a href="#9-6随机初始化" class="headerlink" title="9-6随机初始化"></a>9-6随机初始化</h2><p>　　　这里初始化的是Theta<br>　　　在神经网络里面将Theta设置为0是没有意义的，因为同一层次的各个单元在梯度下降中的值都会一样，这是高度冗余的。<br>　　　所以Theta中的值会被随机化为[-epsilon,epsilon]之间的数。</p>
<h2 id="9-7组合到一起"><a href="#9-7组合到一起" class="headerlink" title="9-7组合到一起"></a>9-7组合到一起</h2><p>　　　最常用的是使用一个隐藏层。多个隐藏层的话需要每个隐藏层的隐藏单元数要相等，正常是输入特征数的1-3倍。隐藏单元数越多越好，但是会加大计算量。<br>　　　步骤：<br>　　　　　　1.随机初始化权重Theta<br>　　　　　　2.使用正向传播计算假设函数的值（激活项）<br>　　　　　　3.计算代价函数<br>　　　　　　4.使用反向传播计算偏导数<br>　　　　　　5.使用梯度检查检测已经得到的偏导数项<br>　　　　　　6.使用梯度下降或者高级优化方法结合反向传递去最小化以Theta为参数的代价函数</p>
<h2 id="9-8无人驾驶"><a href="#9-8无人驾驶" class="headerlink" title="9-8无人驾驶"></a>9-8无人驾驶</h2><p>　　　视频……92年的无人驾驶技术。。</p>
<h2 id="10-1决定下一步做什么"><a href="#10-1决定下一步做什么" class="headerlink" title="10-1决定下一步做什么"></a>10-1决定下一步做什么</h2><p>　　　改正一个学习算法的方法<br>　　　　　　-获得更多的训练样本<br>　　　　　　-使用更小的特征集<br>　　　　　　-使用额外的特征<br>　　　　　　-尝试加入高级幂的特征<br>　　　　　　-选取合适的lambda<br>　　　机器学习诊断<em>Machine learning diagnostic</em><br>　　　　　　检查一个学习算法的运行效果，从而获得指导更好知道如何提高算法的效果。</p>
]]></content>
      <tags>
        <tag>/machineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习_Day6</title>
    <url>/2020/03/05/MLD6/</url>
    <content><![CDATA[<p>吴恩达机器学习-Day_6</p>
<a id="more"></a>

<h2 id="7-1过拟合问题"><a href="#7-1过拟合问题" class="headerlink" title="7-1过拟合问题"></a>7-1过拟合问题</h2><p>　　　过拟合问题<em>the overfitting problem</em>: 函数模型符合已知数据点但无法用于处理未知问题，简单说就是有特殊性但是没有普遍性。<br>　　　正则化<em>regularizatoin</em>: 改善和减少过度拟合。</p>
<h3 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h3><p>　　　<img src="/2020/03/05/MLD6/7-1_P1.png" alt="pic"><br>　　　左边: 欠拟合<em>underfit</em>, 高偏差<em>high bias</em>.<br>　　　右边：过拟合<em>overfit</em>, 高方差<em>high Variance</em>.<br>　　　泛化generalize: 对一般化的数据也同样适用。<br>　　　<img src="/2020/03/05/MLD6/7-1_P2.png" alt="pic"></p>
<h3 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h3><p>　　　1.减少特征数量<br>　　　　　　-人工选择重要的特征<br>　　　　　　-模型选择算法(后面提到)<br>　　　2.正则化<br>　　　　　　-保留所有的特征，但减少量级magnitude或者参数theta的大小<br>　　　　　　-在有很多特征的情况下工作较好，每个特征会为预测y提供一点帮助。</p>
<h2 id="7-2代价函数"><a href="#7-2代价函数" class="headerlink" title="7-2代价函数"></a>7-2代价函数</h2><p>　　　<img src="/2020/03/05/MLD6/7-2_P1.png" alt="pic"><br>　　　对部分参数加入惩罚项，在图中所示，给后面两个参数增加惩罚项减少其对假设函数的影响，使函数更趋近于二次函数。</p>
<p>　　正规化:<br>　　　减少参数theta的值<br>　　　　　　-简化假设函数<br>　　　　　　-防止发生过拟合<br>　　　在代价函数加入惩罚项<br>　　　<img src="/2020/03/05/MLD6/7-2_P1.png" alt="pic"><br>　　　代价函数 $$J\left(\theta\right) = \frac{1}{2m} \left[\sum_{i=1}^m{\left(h_\theta\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)}^2 +\lambda \sum_{j=1}^n{\theta_j^2}\right]$$<br>　　　注意后面的惩罚项是从j=1开始，不包含$\theta_0$。其中$\lambda$叫做正规化系数，如果正规化系数过大会导致”惩罚”过重，最后假设函数收敛到一条和x轴平行的直线y = $\theta_0$。</p>
<h2 id="7-3线性回归的正则化"><a href="#7-3线性回归的正则化" class="headerlink" title="7-3线性回归的正则化"></a>7-3线性回归的正则化</h2><p>　　　梯度下降<br>$$ θ_0 := θ_0 - \alpha \frac{1}{m} \sum_{i=1}^m{\left(h_θ\left(x^{i}\right)-y\right)}x_0^{\left(i\right)} $$<br>$$θ_j := θ_j - \alpha \frac{1}{m} \sum_{i=1}^m{\left(h_θ\left(x^{i}\right)-y\right)}x^{\left(i\right)}  + \frac{\lambda}{m}\theta_j$$<br>　　　上面的式子也可以写成<br>$$θ_j := θ_j\left(1-\alpha\frac{\lambda}{m}\right) - \alpha \frac{1}{m} \sum_{i=1}^m{\left(h_θ\left(x^{i}\right)-y\right)}x^{\left(i\right)}$$<br>　　　其中由于$\alpha$的值很小，所以$θ_j\left(1-\alpha\frac{\lambda}{m}\right)$的值解仅1。<br>　　　<img src="/2020/03/05/MLD6/7-3_P1.png" alt="pic"><br>　　　如果样本数小于特征数，则会导致矩阵退化（即不可逆矩阵）<br>　　　PS: 这个方程使用过代价函数对theta求导后结果为0推出。</p>
<h2 id="7-4逻辑回归的正则化"><a href="#7-4逻辑回归的正则化" class="headerlink" title="7-4逻辑回归的正则化"></a>7-4逻辑回归的正则化</h2><p>　　　加入正则化项的逻辑回归模型的代价函数<br>　　　<img src="/2020/03/05/MLD6/7-4_P1.png" alt="pic"><br>　　　代价函数即为原来的代价函数加上正则化项$\frac{\lambda}{2m}\sum_{j=1}^n{\theta_j^2}$<br>　　　加入正则化项的逻辑回归模型的梯度下降<br>　　　<img src="/2020/03/05/MLD6/7-4_P2.png" alt="pic"><br>　　　弹幕里有人提到为什么后面的theta不用累加的问题，因为每次都是对$theta_j$求偏导，其它下标不是j的在求导的时候就变成0了，所以不是和代价函数一样求和。</p>
<h3 id="高级优化"><a href="#高级优化" class="headerlink" title="高级优化"></a>高级优化</h3><p>　　　<img src="/2020/03/05/MLD6/7-4_P3.png" alt="pic"><br>　　　这节课后面再coursera有个练习==，有时间再做吧</p>
<h2 id="8-1非线性假设"><a href="#8-1非线性假设" class="headerlink" title="8-1非线性假设"></a>8-1非线性假设</h2><p>　　　<img src="/2020/03/05/MLD6/8-1_P1.png" alt="pic"><br>　　　如果特征数量过多，便会导致产生的多项式项数过多，会发生过拟合，但是如果通过减少次数来减少项数又会减少相关项导致曲线欠拟合。<br>　　　由于计算机看到的只是代表亮度的矩阵，那么为了使计算机有识别能力，需要向学习算法提供车和非车的样本集，以下是原理示例。<br>　　　在车上取某两个位置的像素点，根据亮度值标注在图上，车和非车也同样如此处理，如下图所示：<br>　　　<img src="/2020/03/05/MLD6/8-1_P2.png" alt="pic"></p>
<h2 id="8-2神经元和大脑"><a href="#8-2神经元和大脑" class="headerlink" title="8-2神经元和大脑"></a>8-2神经元和大脑</h2><p>　　　神经网络<em>neural networks</em>起源: 尝试去模拟人的大脑的算法。<br>　　　盛行于80年代和90年代早期，但是没落与90年代末，由于计算速度提高在近阶段兴盛。<br>　　　生理学上的证据：切断耳朵和大脑上听觉中枢的神经，接上眼睛后这个大脑区域拥有看的能力。（神经重连实验）</p>
<h2 id="8-3模型展示"><a href="#8-3模型展示" class="headerlink" title="8-3模型展示"></a>8-3模型展示</h2><p>　　　人类是通过神经元进行信息传递。<br>　　　所以在计算机里面，神经网络将逻辑单元当做神经元。<br>　　　Sigmoid(logistic) 激活函数<em>actication funciont</em>。<br>　　　theta参数在某些文献里面也会被叫做权重<em>weight</em><br>　　　可能会有偏置单元<em>bias unit</em>.<br>　　　在神经网络中，第一层也叫作输入层<em>input layer</em>,最后一层叫做输出层<em>output layer</em>, 中间的叫做隐藏层<em>hidden layer</em>.<br>　　　<img src="/2020/03/05/MLD6/8-3_P1.png" alt="pic"><br>　　　$a_i^{\left(j\right)}$=第j层的第i个激活项<br>　　　$\theta^{\left(j\right)}$=权重矩阵控制从j层到j+1等的映射函数。<br>　　　<img src="/2020/03/05/MLD6/8-3_P2.png" alt="pic"></p>
]]></content>
      <tags>
        <tag>/machineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-Day_5</title>
    <url>/2020/03/04/MLD5/</url>
    <content><![CDATA[<p>吴恩达机器学习-Day_5</p>
<a id="more"></a>

<h2 id="6-1分类"><a href="#6-1分类" class="headerlink" title="6.1分类"></a>6.1分类</h2><p>　　　是/否，可以有两个取值的变量<br>　　　$$y\in\left\{0,1\right\}$$<br>　　　0: 负类”Negative Class”(e.g., benign tumor)<br>　　　1: 正类”Positive Class”(e.g., malignant tumor)<br>　　　如果y取值多的叫多分类</p>
<p>　　　<img src="/2020/03/04/MLD5/6-1_P1.png" alt="pic"><br>　　　如果和线性回归一样设假设函数，设置阈值为0.5，即如果假设函数值等于阈值时，对应的横坐标左侧的函数值都视为0，反之视为1.<br>　　　但是这种方法在有极端数据之后，假设函数在图像上的斜率变小，从而当函数值等于阈值时，有很多呈阳性的肿瘤被判断成阴性（在阈值对应的横坐标的左侧），从而造成错误的预测。所以线性回归不适合运用在分类问题。<br>　　　而且使用线性回归时，假设函数值可能会远大于1或者远小于0，这显得很奇怪。<br>　　　（个人感觉并不奇怪，因为有设定一个阈值，关键问题在于线性回归会受到偏于优势的数据的影响，类如上面那个例子，设肿瘤体积小于2为分类标准，如果数据点集中在小于2的部分，则会斜率增加，把原本属于阴性的误判为阳性的，同理对阳性也是如此）</p>
<p>　　　<strong>逻辑回归Logistic Regression</strong>可以保证加黑色函数值能够保持在[0,1]</p>
<h2 id="6-2假设陈述"><a href="#6-2假设陈述" class="headerlink" title="6-2假设陈述"></a>6-2假设陈述</h2><p>　　　原来的线性回归方程假设为$h_\theta\left(x\right) = \theta^Tx$<br>那么逻辑方程就令$$h_\theta\left(x\right) = g(\theta^Tx)$$<br>$$g(z) = \frac{1}{1+e^{-z}}$$<br>　　　<strong>Sigmoid function = Logistic function</strong>都指代g函数<br>$$h_\theta\left(x\right) = \frac{1}{1+e^{-\theta^Tx}}$$<br>　　　<img src="/2020/03/04/MLD5/6-1_P2.png" alt="pic"><br>　　　假设函数的图像大约如上图所示。</p>
<p>　　　在逻辑回归方程里的假设函数$h_\theta\left(x\right)$表示估计当x取某值时y=1的概率。<br>　　　即概率论里的条件概率，也可以写为$P\left(y=0|x;\theta\right)$</p>
<h2 id="6-3决策界限"><a href="#6-3决策界限" class="headerlink" title="6-3决策界限"></a>6-3决策界限</h2><h3 id="线性决策界限"><a href="#线性决策界限" class="headerlink" title="线性决策界限"></a>线性决策界限</h3><p>　　　<img src="/2020/03/04/MLD5/6-3_P1.png" alt="pic"><br>　　　决策界限decision boundary: 假设函数对应的直线，按照其函数值小于/大于阈值将图像分成两个部分就像分界线一样(如图中红色线)。<br>　　　<strong>决策界限不是训练集的属性，而是假设函数的属性。</strong></p>
<h3 id="非线性决策界限"><a href="#非线性决策界限" class="headerlink" title="非线性决策界限"></a>非线性决策界限</h3><p>　　　<img src="/2020/03/04/MLD5/6-3_P2.png" alt="pic"><br>　　　其实就像回归方程，除了要找出合适的参数theta，还要找到合适的多项式方程。</p>
<h2 id="6-4代价函数"><a href="#6-4代价函数" class="headerlink" title="6-4代价函数"></a>6-4代价函数</h2><p>　　　<img src="/2020/03/04/MLD5/6-4_P1.png" alt="pic"><br>　　　在logicstic回归的代价函数和线性回归的代价函数不同，原因在于逻辑回归的假设函数时非线性的，会导致代价函数有多个局部最小值。<br>　　　在逻辑回归的代价函数将被写为<br>$$J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}{Cost\left(h_\theta\left(x\right),y\right)}$$<br>　　　<img src="/2020/03/04/MLD5/6-4_P2.png" alt="pic"><br>　　　<img src="/2020/03/04/MLD5/6-4_P3.png" alt="pic"><br>　　　对y=1来说，如果假设函数值为1，那么cost为0，而假设函数越趋于0，则代价越高，即如果我们计算得到假设函数值为0，而y却为1，则我们用一个很大的代价”惩罚”这个学习算法。</p>
<h2 id="6-5简化代价函数和梯度下降"><a href="#6-5简化代价函数和梯度下降" class="headerlink" title="6-5简化代价函数和梯度下降"></a>6-5简化代价函数和梯度下降</h2><p>　　　上面提到的代价函数需要分为y=0和y=1的情况，但是可以再简化为<br>　　　<img src="/2020/03/04/MLD5/6-5_P1.png" alt="pic"><br>　　　似乎可以采用其它代价函数，但是这种形式是从极大似然法maximum likelihood estimation得来的。<br>　　　<img src="/2020/03/04/MLD5/6-5_P2.png" alt="pic"></p>
<h2 id="6-6高级优化"><a href="#6-6高级优化" class="headerlink" title="6-6高级优化"></a>6-6高级优化</h2><p>　　　优化算法：<strong>Conjugate  gradient, BFGS, L-BFGS</strong><br>　　　优点：不需要认为选择学习率，算法内部的智能内循环(线搜索算法);常常快于梯度下降算法。<br>　　　缺点：太复杂。<br>　　　<img src="/2020/03/04/MLD5/6-6_P1.png" alt="调用优化算法"></p>
<h2 id="6-7多元分类：一对多"><a href="#6-7多元分类：一对多" class="headerlink" title="6-7多元分类：一对多"></a>6-7多元分类：一对多</h2><p>　　　<img src="/2020/03/04/MLD5/6-7_P1.png" alt="pic"><br>　　　需要n个分类器来分出n个种类，每个分类器可以分出代价最小的种群。（有个疑问：会不会有某个或多个样本同时处在多个类中？）</p>
]]></content>
      <tags>
        <tag>/machineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习编程作业-1</title>
    <url>/2020/03/03/ML-ex1/</url>
    <content><![CDATA[<p>吴恩达机器学习习题ex1解答<br>原题可以在这里找到，需要用邮箱注册，不需要翻墙<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">coursera学习网站，类似中国的mooc</a></p>
<a id="more"></a>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>　　　这个网站最多只能用来做做题还有看看课程资料了，课程视频是看不了的，估计被墙了……虽然官网有中文，但是所有的课程资料都是英文……<br>　　　在学习到单变量线性回归方程的时候会有一个编程作业，下载一个zip文件，里面有作业介绍ex1.pdf,还有作业文件，作业包含必做题和选做题，必做题考察基础的工具操作和单变量线性回归方程，选做题涉及特征缩放，多变量线性回归方程，正规化。只要做完必做题就满分了。<br>　　　至于作业介绍ex1.pdf花了我将近一个小时才看懂操作方式……首先要把文件解压，然后把路径调整到这个文件夹下，我用的是matlab可以通过UI界面进入，命令行的话要通过cd命令跳转过去。<br>　　　<img src="/2020/03/03/ML-ex1/ex1_P1.png" alt="pic"><br>$$<br>\begin{array}{l|r}<br>{文件名}&amp;{功能}\\<br>\hline<br>{ex1.m}&amp;{必做题的运行脚本}\\<br>{ex1_multi.m}&amp;{选做题的运行脚本}\\<br>{submit.m}&amp;{提交的运行脚本}\\<br>{下面的文件}&amp;{星号是必做题，加号是选做题}\\<br>\end{array}<br>$$</p>
<h2 id="必做题"><a href="#必做题" class="headerlink" title="必做题"></a>必做题</h2><p>　　　首先先要根据注释的instruction写出必做题中的代码。<br>　　　第一题，是输出一个5*5的单位矩阵。</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">A = <span class="built_in">eye</span>(<span class="number">5</span>)</span></pre></td></tr></table></figure>

<p>　　　第二题，是将数据点显示在图上。</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="built_in">plot</span>(x, y, <span class="string">'rx'</span>, <span class="string">'MarkerSize'</span>, <span class="number">10</span>);  </span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">% rx表示用红色x来画出数据点，MarkSize设置大小</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">% 如果没有rx的话所有数据点会连一起而不是单独画出</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">xlabel(<span class="string">'Population of City in 10,000s'</span>);</span></pre></td></tr><tr><td class="code"><pre><span class="line">ylabel(<span class="string">'Profit in $10,000s'</span>);</span></pre></td></tr></table></figure>

<p>　　　第三题，计算代价函数和梯度下降<br>　　　代价函数的公式$ \mathtt{J} \left( θ_0,θ_1x \right) = \frac {1}{2m} \sum_{i=1}^m {\left( h_θ \left( x^{i} \right ) -y \right)} ^2 $</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">J = sum((X * theta - y).^<span class="number">2</span>) / (<span class="number">2</span>*m);</span></pre></td></tr></table></figure>

<p>　　　这边要注意两个东西，一个是theta是行向量，而之前说的$\theta^Tx$是theta是列向量才需要翻转。那么根据矩阵相乘的知识，要么用 theta.<em>X,要么用X</em>theta，两者都是为了生成97*1的向量来跟y相减，最后这个平方号要注意是向量内的每一个元素平方，而不是向量乘向量。</p>
<p>　　　梯度下降的公式<br>　　　$ θ_0 := θ_0 - \alpha \frac{1}{m} \sum_{i=1}^m{\left(h_θ\left(x^{i}\right)-y\right)}^2 $<br>　  $ θ_0 := θ_0 - \alpha \frac{1}{m} \sum_{i=1}^m{\left(h_θ\left(x^{i}\right)-y\right)}^2 \dot x^{\left(i\right)}$ </p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">theta_s = theta;</span></pre></td></tr><tr><td class="code"><pre><span class="line">theta_s(<span class="number">1</span>) = theta(<span class="number">1</span>) - alpha / m * sum(X * theta - y);</span></pre></td></tr><tr><td class="code"><pre><span class="line">theta_s(<span class="number">2</span>) = theta(<span class="number">2</span>) - alpha / m * sum((X * theta - y) .* X(:,<span class="number">2</span>));</span></pre></td></tr><tr><td class="code"><pre><span class="line">theta=theta_s;</span></pre></td></tr></table></figure>

<p>　　　注意$\theta_0和\theta_1$是同步更新的，所以设置了一个暂存量theta_s.</p>
<p>　　　第四题是代价函数的可视化，不需要额外填写代码。</p>
<h2 id="提交"><a href="#提交" class="headerlink" title="提交"></a>提交</h2><p>　　　在对应的文件完成代码之后，在命令行窗口输入ex1看看是否正确运行，运行的结果可以和ex1.pdf进行比较，若结果正确，则在命令行窗口输入submit提交，输入邮箱和识别码(在coursera的作业界面领取)，即可提交并打分，并且将结果上传，凡是写nice work!的都是表示结果正确。</p>
]]></content>
      <tags>
        <tag>/machineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-Day_4</title>
    <url>/2020/03/03/MLD4/</url>
    <content><![CDATA[<p>吴恩达机器学习笔记第四天</p>
<a id="more"></a>
<h2 id="5-1基本操作"><a href="#5-1基本操作" class="headerlink" title="5.1基本操作"></a>5.1基本操作</h2><p>　　　由于原视频使用的是Octave,然后我自己有现成的matlab能用，根据实践发现两者操作是共通的。</p>
<h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><p>　　　基本数学操作（加减乘除），<strong>不等于号是~=</strong>,逻辑运算。<br>　　　用分号可以阻止输出。还有用经典的C语言<strong>注意是sprintf</strong>控制输出格式。<br>　　　format long\short 输出默认的长\短位。</p>
<h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><p>　　　A = [1 2;3 4;5 6]可以生成如下的矩阵<br>$$<br>\begin{bmatrix}<br>1&amp;2\\\<br>3&amp;4\\\<br>5&amp;6<br>\end{bmatrix}<br>$$<br>　　　v = [1 2 3]生成行向量     [1; 2; 3]生成列向量<br>　　　v = 1:0.1:2 生成一个行向量，范围[1, 2], 间隔为0.1<br>　　　当然 [1 2 3] 也可以用类如 1:6 来代替（默认间隔为1）</p>
<p>　　　ones(x, y) 生成x行y列，元素为1的矩阵<br>　　　zeros(x, y) 生成x行y列，元素为0的矩阵<br>　　　rand(x, y) 生成x行y列，元素为（0，1）随机数的矩阵<br>　　　randn(x, y) 生成x行y列，元素为满足高斯分布的随机数的矩阵<br>　　　eye(x) 画出x行x列的单位矩阵</p>
<p>　　　hist(x) 画出x的分布直方图</p>
<h2 id="5-2移动数据"><a href="#5-2移动数据" class="headerlink" title="5.2移动数据"></a>5.2移动数据</h2><p>　　　size(A) 返回矩阵A的大小<br>　　　size(A, 1) 返回第一维度的大小（即行数）<br>　　　size(A, 2) 返回第二维度的大小（即列数）<br>　　　length(v) 返回向量v的大小<br>　　　length(A) 返回矩阵A的行数</p>
<p>　　　load xxx 表示读取xxx文件<br>　　　save xxx y 表示把数据y存入xxx文件中<br>　　　在后面加入 -ascii 可以让它用二进制的方式储存</p>
<p>　　　A(3,2) 表示第三行第二列的元素<br>　　　A(2,:) :表示取该维度所有值，这表示取了第二行的所有值<br>　　　A([1 3], :) 表示去了第1、3行的所有值<br>　　　A = [A, [100; 101; 102]] 在A的右边添加一列<br>　　　A(:) 标A的所有值放入一个列向量</p>
<h2 id="5-3计算数据"><a href="#5-3计算数据" class="headerlink" title="5.3计算数据"></a>5.3计算数据</h2><p>　　　A<em>B表示矩阵相乘， A.</em>B表示矩阵内的元素相乘<br>　　　两个区别是，矩阵相乘要第一个矩阵的列数等于第二个矩阵的行数<br>　　　而元素相乘要保证矩阵大小相同</p>
<p>　　　A’: 求矩阵A的转置<br>　　　max(A): 返回A每一行内最大的数和index索引<br>　　　a &lt; 3: 类似这种的条件语句，返回一个bool矩阵返回每个元素&lt;3的结果<br>　　　find(a &lt; 3): 和上面相同，一个是返回boo值，一个是返回元素值<br>　　　magic(A): 幻方，返回行列斜线的和相同的矩阵
　　　
　　　</p>
]]></content>
      <tags>
        <tag>/machineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-Day_2&amp;Day_3</title>
    <url>/2020/03/01/MLD2/</url>
    <content><![CDATA[<p>吴恩达机器学习笔记D2&amp;D3</p>
<a id="more"></a>
<h2 id="3-1矩阵与向量"><a href="#3-1矩阵与向量" class="headerlink" title="3.1矩阵与向量"></a>3.1矩阵与向量</h2><p>　　　Matrix: Rectangular array of numbers.<br>　　　Dimension of matrix: number of rows times number of cols.<br>　　　$A_{ij}$ = “i,j entry” in the $i^{th}$ row, $j^{th}$ col.<br>　　　Vector: An n*1 matrix(在线代里还有细分行向量和列向量)<br>　　　$y_i = i^{th} element$</p>
<h2 id="3-2加法和标量乘法"><a href="#3-2加法和标量乘法" class="headerlink" title="3.2加法和标量乘法"></a>3.2加法和标量乘法</h2><h3 id="矩阵加法"><a href="#矩阵加法" class="headerlink" title="矩阵加法"></a>矩阵加法</h3><p>$$<br>\begin{bmatrix}<br>1&amp;0\\\<br>2&amp;5\\\<br>3&amp;1<br>\end{bmatrix}+<br>\begin{bmatrix}<br>4&amp;0.5\\\<br>2&amp;5\\\<br>0&amp;1<br>\end{bmatrix}=<br>\begin{bmatrix}<br>5&amp;0.5\\\<br>4&amp;10\\\<br>3&amp;2<br>\end{bmatrix}<br>$$  </p>
<h3 id="标量乘法"><a href="#标量乘法" class="headerlink" title="标量乘法"></a>标量乘法</h3><p>$$<br>3 \ast<br>\begin{bmatrix}<br>1&amp;0\\\<br>2&amp;5\\\<br>3&amp;1<br>\end{bmatrix}=<br>\begin{bmatrix}<br>3&amp;0\\\<br>6&amp;15\\\<br>9&amp;3<br>\end{bmatrix}<br>$$</p>
<h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><p>　　　<img src="/2020/03/01/MLD2/3-6_P1.png" alt="pic"><br>　　　和列方程组差不多，左边是输入，经过第二个矩阵的变换（假设函数）输出结果。</p>
<h3 id="3-5矩阵乘法的特征"><a href="#3-5矩阵乘法的特征" class="headerlink" title="3.5矩阵乘法的特征"></a>3.5矩阵乘法的特征</h3><h3 id="3-6逆和转置"><a href="#3-6逆和转置" class="headerlink" title="3.6逆和转置"></a>3.6逆和转置</h3><h2 id="4-1多元线性回归的多特征"><a href="#4-1多元线性回归的多特征" class="headerlink" title="4.1多元线性回归的多特征"></a>4.1多元线性回归的多特征</h2><p>　　　<img src="/2020/03/01/MLD2/4-1_P1.png" alt="pic"><br>　　　n是特征的数量<br>　　　$x^{\left(i\right)}$表示第i个样本<br>　　　$x^{\left(i\right)}_j$表示第i个样本中第j个特征的值</p>
<h3 id="Hypothesis"><a href="#Hypothesis" class="headerlink" title="Hypothesis"></a>Hypothesis</h3><p>　　　过去是单变量的线性回归，随着特征量的增加，需要对假设函数修改<br>　　　$$h_{\theta}\left(x\right) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3$$<br>　　　这里的x0的取值默认取1，之所以这么取是可能是为了向量的纬度对齐。<br>　　　于是便可以写成如下形式<br>$$<br>x =<br>\begin{bmatrix}<br>x_0\\\<br>x_1\\\<br>x_2\\\<br>\vdots \\\<br>x_n<br>\end{bmatrix}<br><br>\theta =<br>\begin{bmatrix}<br>\theta_0\\\<br>\theta_1\\\<br>\theta_2\\\<br>\vdots\\\<br>\theta_n<br>\end{bmatrix}<br>$$<br>$$ h_{\theta}\left(x\right) = \theta^Tx$$<br>　　　这里要注意内外积的区别，因为结果是一个标量而不是一个矩阵，所以是前面的theta转置。</p>
<h2 id="4-2多元梯度下降法"><a href="#4-2多元梯度下降法" class="headerlink" title="4.2多元梯度下降法"></a>4.2多元梯度下降法</h2><p>　　　假设函数: $h_{\theta}\left(x\right) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3$<br>　　　参数: $\theta_0,\theta_1,\cdots,\theta_n$<br>　　　代价函数:<br>$$J\left(\theta_0,\theta_1,\cdots,\theta_n\right)=\frac {1}{2m} \sum_{i=1}^m {\left( h_θ \left( x^{i} \right) -y \right)} ^2$$<br>　　　梯度下降<br>　　　$θ_j := θ_0 - α\frac{∂}{∂θ_j}J\left(\theta_0,\theta_1,\cdots,\theta_n\right)$<br>　　　<img src="/2020/03/01/MLD2/4-2_P1.png" alt="pic"><br>　　　常数项1声明为了$x_0$，所以$\theta_0$的情况也包含了进去。</p>
<h2 id="4-3特征缩放"><a href="#4-3特征缩放" class="headerlink" title="4.3特征缩放"></a>4.3特征缩放</h2><h3 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h3><p>　　　特征缩放Feature Scaling: make sure feature are on a similar scale.<br>　　　<img src="/2020/03/01/MLD2/4-3_P1.png" alt="pic"><br>　　　由于卧室数量和房子面积比例为400:1, 所以会导致在画代价函数图像的时候画出的椭圆太扁平。为了方便收敛，采取右边的对特征量进行缩放的方法。<br>　　　看弹幕提到一个问题，这个椭圆是竖着还是躺着，原图这么画是没错的，因为x1的范围相对于x2较大，所以导致$\theta_1$稍微变化一点，代价函数的值就会有更大的变化，这就是为什么在$\theta_1$轴上，等高线如此密集的原因。<br>　　　方法：尽量让所有的特征大致收缩在[-1,1]的范围之间，当然像[0,2]这样相对较小的也可以。</p>
<h3 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h3><p>　　　均值归一化Mean normalization: Replace $x_i$ with $x_i-\mu_i$ to make features have approximately zero mean.(Don`t apply to $x_0$ = 1)<br>　　　上面这些是为了把特征值$x_i = \frac{x_i-\mu_i}{S_i}$<br>　　　$\mu_i$表示平均值，而$S_i$可以用样本集的方差或者极差(最大值减最小值)。<br>　　　以上的特征缩放是为了减少迭代次数，加快梯度下降的速度。</p>
<h2 id="4-4学习率"><a href="#4-4学习率" class="headerlink" title="4.4学习率"></a>4.4学习率</h2><p>　　　两个目标:<br>　　　1.”Debugging”: 如何保证梯度下降正常工作<br>　　　2.如何选择学习率α</p>
<h3 id="判断梯度下降正常工作"><a href="#判断梯度下降正常工作" class="headerlink" title="判断梯度下降正常工作"></a>判断梯度下降正常工作</h3><p>　　　<img src="/2020/03/01/MLD2/4-4_P1.png" alt="pic"><br>　　　画出最小代价函数值随着迭代次数变化的曲线图。随着迭代次数的上升代价函数逐渐收敛到一个最小值。</p>
<h3 id="自动收敛测试"><a href="#自动收敛测试" class="headerlink" title="自动收敛测试"></a>自动收敛测试</h3><p>　　　automatic convergence test自动收敛测试: 如果在一次迭代中，J(θ)小于某个很小的数(如1e-3)，则视为函数已收敛。<br>　　　但是这个阈值时很难确定的，所以从上面的曲线图判断是否收敛比较好。</p>
<h3 id="代价函数不随着迭代次数下降的异常情况"><a href="#代价函数不随着迭代次数下降的异常情况" class="headerlink" title="代价函数不随着迭代次数下降的异常情况"></a>代价函数不随着迭代次数下降的异常情况</h3><p>　　　<img src="/2020/03/01/MLD2/4-4_P2.png" alt="pic"><br>　　　1. 选取适合的学习率，代价函数会随着迭代次数上升下降<br>　　　2. 如果学习率过低，导致迭代次数过多<br>　　　3. 如果学习率过高，代价函数可能不会在迭代中减少或收敛。</p>
<h2 id="4-5特征和多项式回归"><a href="#4-5特征和多项式回归" class="headerlink" title="4.5特征和多项式回归"></a>4.5特征和多项式回归</h2><p>　　　1. 特征量的选取要选取真正影响假设函数的特征。<br>　　　2. 多项式的选取也要根据数据的分布选取。（如二次函数随着x增加会先升后降，三次函数则会x增加升降升）</p>
<h2 id="4-6正规方程（区别于迭代方法的直接解法）"><a href="#4-6正规方程（区别于迭代方法的直接解法）" class="headerlink" title="4.6正规方程（区别于迭代方法的直接解法）"></a>4.6正规方程（区别于迭代方法的直接解法）</h2><p>　　　<img src="/2020/03/01/MLD2/4-6_P1.png" alt="pic"><br>　　　一个方法是，对每一个θ进行求偏导数，并将导数置零（即求出能够让代价函数最小的时候θ的取值）。</p>
<p>　　　另一个正规方程， 即列出矩阵方程求出参数，具体过程如下<br>　　　<img src="/2020/03/01/MLD2/4-6_P2.png" alt="pic"><br>　　　(这边就是如同Ax=y这种矩阵方程的求解)<br>　　　如果使用特征方程的话则不需要进行特征缩放。</p>
<h3 id="梯度下降-即迭代方法-和正规方程的比较"><a href="#梯度下降-即迭代方法-和正规方程的比较" class="headerlink" title="梯度下降(即迭代方法)和正规方程的比较"></a>梯度下降(即迭代方法)和正规方程的比较</h3><p>$$<br>\begin{array}{l|l}<br>{Gradient　Descent}&amp;{Birnak　Equation}\\<br>\hline<br>{Need　to　choose　\alpha}&amp;{No　need　to　choose　\alpha}\\<br>{Need　many　iterations}&amp;{Don’t　need　to　iterate}\\<br>{Works　well　even　when　n　is　large}&amp;{Need　to　compute　\left(X^TX\right)^{-1}　and　slow　if　n　is　large　O\left(n^3\right)}\\<br>\end{array}<br>$$</p>
<h2 id="4-7正规方程在矩阵不可逆情况下的解决方法"><a href="#4-7正规方程在矩阵不可逆情况下的解决方法" class="headerlink" title="4.7正规方程在矩阵不可逆情况下的解决方法"></a>4.7正规方程在矩阵不可逆情况下的解决方法</h2><p>　　　Otacle中的Pinv：Pseudo-inverse伪逆（我也没听说过）<br>　　　不可逆non-invertible有两种情况:<br>　　　1. 线性相关的特征(重复的特征)<br>　　　　　　-删除重复的特征<br>　　　2. 特征量过多，导致样本数少于特征数量。<br>　　　　　　-删除一些影响小的特征，或则使用正规化regularization。</p>
]]></content>
      <tags>
        <tag>/machineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>博客下划线数学渲染成斜体</title>
    <url>/2020/03/01/%E5%8D%9A%E5%AE%A2%E4%B8%8B%E5%88%92%E7%BA%BF%E6%95%B0%E5%AD%A6%E6%B8%B2%E6%9F%93%E6%88%90%E6%96%9C%E4%BD%93/</url>
    <content><![CDATA[<p>今天回看上次的笔记，发现数学渲染的一塌糊涂……<br>在博客找到解决方法<br><a href="https://segmentfault.com/a/1190000007261752" target="_blank" rel="noopener">Hexo下mathjax的转义问题</a><br>博客中提到两个，一个是更换markdown的渲染引擎，一个是更换mathhax的渲染引擎，另一个是更改渲染规则，我用的是第三个，路径在/nodes_modules/marked/lib/marked.js<br>ps: 我这边是在var inline的代码块里的，可能修改前内容和博客里的不一样，但是修改后的内容填进去后在博客里确实渲染成功了。<br><a href="https://www.jianshu.com/p/8b6fc36035c0" target="_blank" rel="noopener">Markdown-常用数学公式编辑命令</a><br>pps: 查的另一个博客的公式渲染都挂了，这么博主是什么神仙……  </p>
]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-Day_1</title>
    <url>/2020/02/29/MLD1/</url>
    <content><![CDATA[<p><a href="https://www.bilibili.com/video/av50747658?from=search&seid=16348381955975479986" target="_blank" rel="noopener">视频链接</a>  </p>
<p>关于吴恩达机器学习的笔记</p>
<a id="more"></a>
<h2 id="第一节"><a href="#第一节" class="headerlink" title="第一节"></a>第一节</h2><h3 id="机器学习的概述"><a href="#机器学习的概述" class="headerlink" title="机器学习的概述"></a>机器学习的概述</h3><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><h4 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h4><p>　　如搜索引擎，医疗记录和生物工程等等</p>
<h4 id="无法人工编程的项目"><a href="#无法人工编程的项目" class="headerlink" title="无法人工编程的项目"></a>无法人工编程的项目</h4><p>　　自动驾驶，字迹识别，自然语言理解ＮＬＰ和计算机视觉ＣＶ</p>
<h4 id="个人定制推荐"><a href="#个人定制推荐" class="headerlink" title="个人定制推荐"></a>个人定制推荐</h4><h2 id="第二节"><a href="#第二节" class="headerlink" title="第二节"></a>第二节</h2><h3 id="定义机器学习"><a href="#定义机器学习" class="headerlink" title="定义机器学习"></a>定义机器学习</h3><h4 id="Arthur-Samuel’s-Definition"><a href="#Arthur-Samuel’s-Definition" class="headerlink" title="Arthur Samuel’s Definition"></a>Arthur Samuel’s Definition</h4><p>　　Fleid of study that gives computer the ability to learn without being explicitly programmed.</p>
<h4 id="Tom-Mitchell"><a href="#Tom-Mitchell" class="headerlink" title="Tom Mitchell"></a>Tom Mitchell</h4><p>　　Well-posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experence E.</p>
<h4 id="example"><a href="#example" class="headerlink" title="example"></a>example</h4><p>　　对于某个垃圾邮件过滤系统， 分类出有无用的邮件是T，学习你对邮件的标记是E， 而被分类出的邮件的数量为P。</p>
<h3 id="机器学习的分类"><a href="#机器学习的分类" class="headerlink" title="机器学习的分类"></a>机器学习的分类</h3><h4 id="监督学习supervised"><a href="#监督学习supervised" class="headerlink" title="监督学习supervised"></a>监督学习supervised</h4><p>　　人教会计算机做某事</p>
<h4 id="无监督学习unsupervised"><a href="#无监督学习unsupervised" class="headerlink" title="无监督学习unsupervised"></a>无监督学习unsupervised</h4><p>　　<br>　　让机器自己学习</p>
<h4 id="其他一些词"><a href="#其他一些词" class="headerlink" title="其他一些词"></a>其他一些词</h4><p>　　强化学习Reinforcement learning<br>　　推荐系统recommender systems  
　　</p>
<h2 id="第三节"><a href="#第三节" class="headerlink" title="第三节"></a>第三节</h2><h3 id="举例：房价预测"><a href="#举例：房价预测" class="headerlink" title="举例：房价预测"></a>举例：房价预测</h3><p>　　<img src="/2020/02/29/MLD1/2_P1.png" alt="pic"><br>　　给出数据集，用函数拟合数据分布。问题在于用什么模型拟合数据。</p>
<h3 id="监督学习定义"><a href="#监督学习定义" class="headerlink" title="监督学习定义"></a>监督学习定义</h3><p>　　we gave the algorithm a data set in which the “right answers” were given, That is, we gave it a data set of houses, in which for every example in this data set, we told it what is the right price, the task of the algorithm is to prodeve more of these tight answers.</p>
<h4 id="回归问题Regressin-problem"><a href="#回归问题Regressin-problem" class="headerlink" title="回归问题Regressin problem"></a>回归问题Regressin problem</h4><p>　　Predict <strong>连续值continuous values</strong> output.</p>
<h3 id="另一例子-预测肿瘤"><a href="#另一例子-预测肿瘤" class="headerlink" title="另一例子:预测肿瘤"></a>另一例子:预测肿瘤</h3><h4 id="分类问题classification-problem"><a href="#分类问题classification-problem" class="headerlink" title="分类问题classification problem"></a>分类问题classification problem</h4><p>　　<img src="/2020/02/29/MLD1/2_P2.png" alt="pic"><br>　　trying to predict a <strong>离散值discrete valued</strong> output.</p>
<h4 id="不止一个feature-attrubite"><a href="#不止一个feature-attrubite" class="headerlink" title="不止一个feature/attrubite"></a>不止一个feature/attrubite</h4><p>　　<img src="/2020/02/29/MLD1/2_P3.png" alt="pic"></p>
<h3 id="无穷量特征"><a href="#无穷量特征" class="headerlink" title="无穷量特征"></a>无穷量特征</h3><p>　　举例：支持向量机算法the support Vector Machine</p>
<h2 id="第四节"><a href="#第四节" class="headerlink" title="第四节"></a>第四节</h2><h3 id="无监督学习定义"><a href="#无监督学习定义" class="headerlink" title="无监督学习定义"></a>无监督学习定义</h3><p>　　给出大量的数据集要求它找出数据的类型结构。</p>
<h4 id="聚类算法cluster-algorithm"><a href="#聚类算法cluster-algorithm" class="headerlink" title="聚类算法cluster algorithm"></a>聚类算法cluster algorithm</h4><p>　　<img src="/2020/02/29/MLD1/3_P1.png" alt="pic"></p>
<h5 id="举例：谷歌新闻"><a href="#举例：谷歌新闻" class="headerlink" title="举例：谷歌新闻"></a>举例：谷歌新闻</h5><p>　　搜索新闻并自动分簇</p>
<h5 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h5><p>　　组织大型计算机集群<br>　　社交网络分析<br>　　细分市场<br>　　天文数据分析</p>
<h4 id="鸡尾酒会"><a href="#鸡尾酒会" class="headerlink" title="鸡尾酒会"></a>鸡尾酒会</h4><p>　　两个人说话，由两个不同位置的麦克风录音。（即声音分离）。<br>　　[W,s,v] = svd(((repmat(sum（x.<em>x,1),size(x,1),1).</em>x)*x’);</p>
<h4 id="工具和环境"><a href="#工具和环境" class="headerlink" title="工具和环境"></a>工具和环境</h4><h5 id="Octave"><a href="#Octave" class="headerlink" title="Octave"></a>Octave</h5><h2 id="2-1模型描述"><a href="#2-1模型描述" class="headerlink" title="2.1模型描述"></a>2.1模型描述</h2><h3 id="（数据集）训练集"><a href="#（数据集）训练集" class="headerlink" title="（数据集）训练集"></a>（数据集）训练集</h3><p>　　<strong>m</strong> = 训练样本数量<br>　　<strong>x</strong>‘s = 输入的变量或特征<br>　　<strong>y</strong>‘s = 输出或目标变量<br>　　(x,y) 为一个输出样本<br>　　$(x^{(i)}, y^{(i)})$ 表示第i个样本  
　　</p>
<h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><p>　　<img src="/2020/02/29/MLD1/2-1_P1.png" alt="pic"><br>　　向学习函数learning algorithm提供训练集training set，由学习函数输出假设函数h hypothesis，h is a func make x map from x to y.</p>
<h4 id="how-do-we-represent-h"><a href="#how-do-we-represent-h" class="headerlink" title="how do we represent h?"></a>how do we represent h?</h4><p>　　$$ h_θ(x) = θ_0 + θ_1x $$<br>　　<strong>the univariate linear regressoin</strong> 单变量线性回归</p>
<h2 id="2-2代价函数-数学定义"><a href="#2-2代价函数-数学定义" class="headerlink" title="2.2代价函数-数学定义"></a>2.2代价函数-数学定义</h2><p>　　$θ_i$: the Parameter of the model 模型参数  </p>
<h3 id="some-diffierent-models"><a href="#some-diffierent-models" class="headerlink" title="some diffierent models"></a>some diffierent models</h3><p>　　<img src="/2020/02/29/MLD1/2-2_P1.png" alt="pic"><br>　　Goal: 找到一个使得$ \frac {1}{2m} \sum_{i=1}^m(h_θ(x^{i})-y)^2 $ 最小的参数  </p>
<h3 id="the-cost-function"><a href="#the-cost-function" class="headerlink" title="the cost function"></a>the cost function</h3><p>　　 $ \mathtt{J} \left( θ_0,θ_1x \right) = \frac {1}{2m} \sum_{i=1}^m {\left( h_θ \left( x^{i} \right ) -y \right)} ^2 $<br>　　 代价函数 also recognized as <strong>squre error cost function</strong>.</p>
<h2 id="2-3-4代价函数-应用"><a href="#2-3-4代价函数-应用" class="headerlink" title="2.3-4代价函数-应用"></a>2.3-4代价函数-应用</h2><h3 id="the-graph-of-two-funtion-hypothesis-amp-amp-cost"><a href="#the-graph-of-two-funtion-hypothesis-amp-amp-cost" class="headerlink" title="the graph of two funtion(hypothesis &amp;&amp; cost)"></a>the graph of two funtion(hypothesis &amp;&amp; cost)</h3><p>　　以下为$θ_0 = 0$的情况<br>　　<img src="/2020/02/29/MLD1/2-2_P2.png" alt="pic"><br>　　h(x)的自变量是给定θ后x的函数，J(θ)是参数θ的函数  </p>
<p>　　以下为不等于0的情况<br>　　由于是两个参数，所以代价函数使用等高线contour figure表示<br>　　<img src="/2020/02/29/MLD1/2-2_P3.png" alt="pic">
　　</p>
<h2 id="2-5梯度下降Gredient-descent"><a href="#2-5梯度下降Gredient-descent" class="headerlink" title="2.5梯度下降Gredient descent"></a>2.5梯度下降Gredient descent</h2><h3 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h3><p>　　1、Start with some initial value<br>　　2、Keep changing to reduce cost func until it end up at a minimum</p>
<h3 id="如何工作"><a href="#如何工作" class="headerlink" title="如何工作"></a>如何工作</h3><p>　　 个人理解:任意选定一个起始点，决定一个最佳下降的方向迈出一步，然后从新起点出发，循环上述过程，直到收敛到局部最低点。  </p>
<h3 id="数学描述"><a href="#数学描述" class="headerlink" title="数学描述"></a>数学描述</h3><p>　　 repeat until convergence{<br>　　 　　 $θ_j := θ_0 - α\frac{∂}{∂θ_j}J\left(θ_0,θ_1\right)　　 \left(for \; j = 0 \; and \; j = 1 \; \right)$<br>　　 }  </p>
<p>　　 α is called <strong>the learning rate</strong> 学习率<br>　　 Correct: Simultaneous update 同时更新<br>　　 <img src="/2020/02/29/MLD1/2-5_P1.png" alt="pic"></p>
<h2 id="2-6梯度下降知识点总结"><a href="#2-6梯度下降知识点总结" class="headerlink" title="2.6梯度下降知识点总结"></a>2.6梯度下降知识点总结</h2><p>　　 $ \frac{∂}{∂θ_j}J\left(θ_0,θ_1\right) $ 就是代价函数在θ点上的斜率。更新的理论如图：<br>　　 <img src="/2020/02/29/MLD1/2-6_P1.png" alt="pic"><br>　　 在上面的图像中，斜率为正数，α一定为正数，故θ向左移动。<br>　　 在下面的图像中，斜率为负数，α为正数，故θ向右移动。两侧都在往最低点收敛。  </p>
<p>　　 <img src="/2020/02/29/MLD1/2-6_P2.png" alt="pic"><br>　　 如果α太大时，梯度下降速度较慢，反之则较快。<br>　　 当达到局部最低点时，斜率为0，θ保持。由导数项和学习率共同控制下降幅度。</p>
<h2 id="2-7线性回归的梯度下降"><a href="#2-7线性回归的梯度下降" class="headerlink" title="2.7线性回归的梯度下降"></a>2.7线性回归的梯度下降</h2><p>　　<img src="/2020/02/29/MLD1/2-7_P1.png" alt="pic"><br>　  <img src="/2020/02/29/MLD1/2-7_P2.png" alt="pic"><br>　  注意右式是求导之后的结果，将假设函数h(x)展开之后会更好理解。<br>　  $ θ_0 := θ_0 - \alpha \frac{1}{m} \sum_{i=1}^m{\left(h_θ\left(x^{i}\right)-y\right)}^2 $<br>　  $ θ_0 := θ_0 - \alpha \frac{1}{m} \sum_{i=1}^m{\left(h_θ\left(x^{i}\right)-y\right)}^2 \dot x^{\left(i\right)}$<br>　  线性回归梯度下降结果是一个凸函数convex function.只有一个全局最优解。<br>　  “Batch” Gradient Descent指每一次梯度下降都使用了所有了训练集。<br>　  BB一句，线性回归方程在概率论有参数估计的方法，老师也说在大数据集中梯度下降才有更好的表现。</p>
]]></content>
      <tags>
        <tag>/machineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>用多张照片拼接成一个照片</title>
    <url>/2020/02/26/%E7%94%A8%E5%A4%9A%E5%BC%A0%E7%85%A7%E7%89%87%E6%8B%BC%E6%8E%A5%E6%88%90%E4%B8%80%E4%B8%AA%E7%85%A7%E7%89%87/</url>
    <content><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>　　朋友的生日快到啦，每年都会想一些奇奇怪怪的东西作为礼物= =，机缘巧合下，在b站看到了用多个图片拼接一个大照片思路 -&gt;<a href="https://www.bilibili.com/video/av90485405" target="_blank" rel="noopener">传送门</a>，这个up用的是一个软件，但是感觉自己也可以用opencv来实现= =，于是便写了这么一个程序……<br>　　想想用朋友的偶像照片拼出朋友的自拍感觉会被打XD<br>　　<a href="https://github.com/Misaka-Mikodo/splice_picture" target="_blank" rel="noopener">源码</a>  </p>
<a id="more"></a>

<h1 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h1><h2 id="获取图片"><a href="#获取图片" class="headerlink" title="获取图片"></a>获取图片</h2><p>　　主要要获取的图片是需要拼接的大图片和被拼接的小图片。我这边打算用朋友的自拍，小图片的获取比较难，小图片越多颜色会更加贴近，我使用的是200张偶像照片作为资源库。一个个从网上下载下来自然很慢，所以需要爬虫爬取一些照片，不会爬取只能求助一下万能的CSDN^^<br>　　-&gt;博客的<a href="https://blog.csdn.net/qq_40774175/article/details/81273198" target="_blank" rel="noopener">传送门</a><br>　　PS：注意在name.txt里填写你要爬取的照片的关键字<br>　　爬取结果如图所示<br>　　<img src="/2020/02/26/%E7%94%A8%E5%A4%9A%E5%BC%A0%E7%85%A7%E7%89%87%E6%8B%BC%E6%8E%A5%E6%88%90%E4%B8%80%E4%B8%AA%E7%85%A7%E7%89%87/file.jpg" alt="爬取结果"></p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>　　简单的思路就是每一张照片都有大致的色调，我把每个照片从几万像素点resize到几百的像素点，计算平均的RBG值，记录在vector里面。其次对大图进行分块，也是找到块中平均的RGB值，填入最接近的找到最接近的RGB值的小图。<br>　　这边有个注意点，如果想做一个高清的拼接图，必须在resize之前就把小图存起来，否则resize后的小图就是像素点……建议调试时用resize之后的小图，等到调试成功的时候再存入原图进行拼接，否则会浪费很多时间在拼接小图上。<br>　　<img src="/2020/02/26/%E7%94%A8%E5%A4%9A%E5%BC%A0%E7%85%A7%E7%89%87%E6%8B%BC%E6%8E%A5%E6%88%90%E4%B8%80%E4%B8%AA%E7%85%A7%E7%89%87/finish.jpg" alt="结果"><br>　　<img src="/2020/02/26/%E7%94%A8%E5%A4%9A%E5%BC%A0%E7%85%A7%E7%89%87%E6%8B%BC%E6%8E%A5%E6%88%90%E4%B8%80%E4%B8%AA%E7%85%A7%E7%89%87/tiger.jpg" alt="比较"> </p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>　　基本上没有什么大问题，小问题像是获取像素点的时候把x和y值搞反了，如果在获取像素点的时候发生了段错误基本上是因为这边–。<br>　　<img src="/2020/02/26/%E7%94%A8%E5%A4%9A%E5%BC%A0%E7%85%A7%E7%89%87%E6%8B%BC%E6%8E%A5%E6%88%90%E4%B8%80%E4%B8%AA%E7%85%A7%E7%89%87/bug.jpg" alt="翻车">  </p>
<h1 id="改进的思路"><a href="#改进的思路" class="headerlink" title="改进的思路"></a>改进的思路</h1><h2 id="关于颜色单一的问题"><a href="#关于颜色单一的问题" class="headerlink" title="关于颜色单一的问题"></a>关于颜色单一的问题</h2><p>　　如果资源库的图片不够多，且大图颜色较为单一时，图像在拼接的时候很容易就出现同一张图片多次出现的情况，这样就不能很好的把所有图片利用起来，这里我想到在每一个小图设置一个优先级priority，使用过一次之后降低它的优先级，允许其他张图片来补充这一块像素，当然为了防止颜色差异过大，仍然需要在一定的误差内。<br>　　例如A图和B图和C图，优先级相同都为0，误差计算公式为 $目前最小误差±优先级$ 某一块的RGB均值与三张图的RGB差分别为1，2，4，那么首先找到差值最小的A图，优先级减低为1，其次再寻找的时候再遍历一次三个图，发现B图是优先级最高，且满足误差范围[0,2],故选择B图。<br>　　第三次寻找误差范围是[0,3],此时没有高优先级的图满足，所以在优先级为1的图中找误差最小的A图，A图优先级变成2，误差范围[0,4],此时C图满足误差范围且优先级最高，故选择C图，这么选择的话既能保证用上了尽量多的图片，又能保证颜色误差不会过大。  </p>
<h2 id="关于识别图像"><a href="#关于识别图像" class="headerlink" title="关于识别图像"></a>关于识别图像</h2><p>　　在找图片对应的时候，用RGB均值差找到相近图像只是一个比较粗略的办法，需要切割到足够小块才能保证和大图相近，但是在现实情况下这么处理很花时间，如果能通过图像识别的方法来处理的话能大大加快对应和拼接的进度，相关知识在下方的博客有介绍：<br>　　<a href="http://www.ruanyifeng.com/blog/2011/07/principle_of_similar_image_search.html" target="_blank" rel="noopener">相似图片搜索的原理</a></p>
]]></content>
      <tags>
        <tag>黑科技</tag>
      </tags>
  </entry>
  <entry>
    <title>学校管理系统</title>
    <url>/2020/02/24/%E5%AD%A6%E6%A0%A1%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h1 id="第一天"><a href="#第一天" class="headerlink" title="第一天"></a>第一天</h1><h2 id="进度"><a href="#进度" class="headerlink" title="进度"></a>进度</h2><p>大概花了一天的时间学习wxss和wxml的语法，就勉强上来写项目了，由于之前有过一些学习的经验吧，所以上手还是挺快的，跳过了组件的介绍，打算之后有遇到再学习。目前第一天的进度是在创建了一个主页面<br><img src="/2020/02/24/%E5%AD%A6%E6%A0%A1%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/main-page.jpg" alt="主页面"><br>其它页面的功能尚未设计，本来计划两天解决，看来是想太多了<br>计划后面的话是一天实现一个页面的功能  </p>
<a id="more"></a>

<h2 id="遇到的一些问题"><a href="#遇到的一些问题" class="headerlink" title="遇到的一些问题"></a>遇到的一些问题</h2><h3 id="git的使用尚不明确"><a href="#git的使用尚不明确" class="headerlink" title="git的使用尚不明确"></a>git的使用尚不明确</h3><p>这边做项目打算用git记录每次项目的进度，并且每天git push到github上，但是目前熟悉的只是基础的add、merge、commit等功能，一旦遇到冲突的话就没办法解决了，虽然教程有说方法，但是打算等遇到了再去看教程</p>
<h3 id="icon的寻找"><a href="#icon的寻找" class="headerlink" title="icon的寻找"></a>icon的寻找</h3><p>推荐一个好的网站<a href="https://www.iconfont.cn/?spm=a313x.7781069.1998910419.d4d0a486a" target="_blank" rel="noopener">阿里巴巴的icon</a>，这里有很多的icon使用  </p>
]]></content>
      <tags>
        <tag>黑科技</tag>
        <tag>微信小程序</tag>
      </tags>
  </entry>
  <entry>
    <title>python上generator 的一些疑惑</title>
    <url>/2020/02/10/python%E4%B8%8Agenerator-%E7%9A%84%E4%B8%80%E4%BA%9B%E7%96%91%E6%83%91/</url>
    <content><![CDATA[<h2 id="Python的Generator"><a href="#Python的Generator" class="headerlink" title="Python的Generator"></a>Python的Generator</h2><p>　　<a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017318207388128" target="_blank" rel="noopener">教程及题目链接</a><br>　　<a href="https://blog.csdn.net/mieleizhi0522/article/details/82142856" target="_blank" rel="noopener">关于yield的博客</a><br>　　目前遇到的一个问题是关于生成器yield的一些疑惑，即教程里那一道杨辉三角</p>
<a id="more"></a>

<h2 id="一开始的理解"><a href="#一开始的理解" class="headerlink" title="一开始的理解"></a>一开始的理解</h2><p>　　<br>　　一开始我认为python的yield是类似压栈存储数据的作用(运行流程类似断点)，具体原理可以参考上面的博客，即yield相当于一次调用的终点，然后下一次是从上面yield的地方继续下去的。<br>　　但是数据上的存储不是简单的数据压栈，因为yield存进去的不是数据而是地址，可以看一下下面这个代码的运行结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triangles</span><span class="params">()</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    L = [<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">yield</span> L</span></pre></td></tr><tr><td class="code"><pre><span class="line">        L += [<span class="number">0</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">n = <span class="number">0</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">results = []</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> triangles():</span></pre></td></tr><tr><td class="code"><pre><span class="line">    results.append(t)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    n = n + <span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">if</span> n == <span class="number">5</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">break</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(results)</span></pre></td></tr></table></figure>

<p>　　运行结果：<br>　　<code>[[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0]]</code><br>　　可以看出results里的所有数据都是同一个元素</p>
<h2 id="另外一个修改后的代码"><a href="#另外一个修改后的代码" class="headerlink" title="另外一个修改后的代码"></a>另外一个修改后的代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triangles</span><span class="params">()</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    L = [<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">yield</span> L</span></pre></td></tr><tr><td class="code"><pre><span class="line">        L = L + [<span class="number">0</span>] <span class="comment">#改动的地方</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">n = <span class="number">0</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">results = []</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> triangles():</span></pre></td></tr><tr><td class="code"><pre><span class="line">    results.append(t)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    n = n + <span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">if</span> n == <span class="number">5</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">break</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(results)</span></pre></td></tr></table></figure>

<p>　　运行结果：</p>
<p>　　<code>[[1], [1, 0], [1, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0, 0]]</code></p>
<h2 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a>个人理解</h2><p>　　python里面的数据都是动态的，L += [0]只是基于同一个地址进行修改，所以在results里面的元素其实都是指向同一个list，而L = L + [0]是形成了一个新的数据，所以在yield的时候导入了“新地址”（至于是否是用一个地址，还是新的地址但是是同一个值，原因无法确定）<br>　　但是这一个结果告诉我们，python里面L = L + [0] 和 L += [0]两个式子并不是完全等价的，这是一个和C语言不同的地方</p>
]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo的第一次安装</title>
    <url>/2019/12/06/Hexo-install/</url>
    <content><![CDATA[<h2 id="选择Hexo的原因"><a href="#选择Hexo的原因" class="headerlink" title="选择Hexo的原因"></a>选择Hexo的原因</h2><p>　　当时单纯是因为一个喜欢的up主<a href="https://space.bilibili.com/384068749?from=search&seid=2197445177359928357" target="_blank" rel="noopener" codesheep的b站空间"">codesheep</a>的一个更新,up讲的挺仔细的，我第一次装+配环境（除了git，我之前就装过了），大概花了我1小时的时间，总之还是挺高效的。<br>　　当然，如果你是第一次想要装博客，选择还是挺多的，像我的同学就是采用了JeKyll去搭建的。-&gt;<a href="https://wu-kan.github.io/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/%E5%9F%BA%E4%BA%8EJekyll%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2" target="_blank" rel="noopener">传送门</a>，不同的框架有不同的优点吧，这边我记录一下在windows10环境下安装hexo的过程。</p>
<a id="more"></a>

<h2 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h2><p>　　首先下载一个nodejs的软件。因为我是windows10 64位系统，所以选择的是图中所高亮的那个安装包。<br><br> <img src="/2019/12/06/Hexo-install/nodejs_install.png" alt="Hexo-install"><br>　　下载之后就可以安装啦。这边注意：如果你要装在其它路径下要先创建好对应的文件夹再放进去，否则会直接把一大堆文件塞进你选定的路径……<br><br> <img src="/2019/12/06/Hexo-install/nodejs_comp.png" alt="Hexo-install">  <img src="/2019/12/06/Hexo-install/nodejs_path.png" alt="Hexo-install"><br>　　安装成功后进去cmd界面查看，若能显示出对应的版本号说明安装成功了。<br>　　<code>node --version</code><br>　　<code>npm --version</code><br><br> <img src="/2019/12/06/Hexo-install/correct.png" alt="Hexo-install"><br>　　接下来使用npm下载cnpm，用的是淘宝的镜像源。<br>　　<code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code><br><br> <img src="/2019/12/06/Hexo-install/cmd_cpm.png" alt="Hexo-install"><br>　　安装成果后输入<code>cnpm --version</code>查看安装情况，若显示出对应的版本号说明安装正确。<br> <img src="/2019/12/06/Hexo-install/cmd_cnpm_comp.png" alt="Hexo-install"><br>　　下载完cnpm后，再用cnpm来下载hexo。首先先输入<code>mkdir blog</code>创建一个文件夹，然后进入该文件夹，在这里要注意，<strong>接下来的所有操作都是在这个文件夹进行的</strong>，完成后就可以输入<code>cnpm install -g hexo-cli</code>进行安装了，安装过程如下：<br><br> <img src="/2019/12/06/Hexo-install/cmd_hexo.png" alt="Hexo-install"><br>　　查看是否安装成功<code>hexo -v</code><br><br> <img src="/2019/12/06/Hexo-install/cmd_hexo_v.png" alt="Hexo-install"><br>　　hexo初始化<code>hexo init</code>……这边是后面在windows装的，git还没下，所以会有这么一个错误<br><br> <img src="/2019/12/06/Hexo-install/cmd_Wrong_git.png" alt="Hexo-install"><br>　　在百度上下载<a href="https://git-scm.com/download/win" target="_blank" rel="noopener">git</a>就可以了，感觉速度有点慢，应该是需要翻墙吧……当然我用我们的校网等了一小段时间下好了，下面是安装界面<br><br> <img src="/2019/12/06/Hexo-install/git_install.png" alt="Hexo-install">　　<br>　　记得记一下安装路径……我安装后它没有给我配置系统路径，所以需要自己配一下。<br><br> <img src="/2019/12/06/Hexo-install/git_set_Patg.png" alt="Hexo-install">　　<br><br> <img src="/2019/12/06/Hexo-install/sys_path.png" alt="Hexo-install"><br>　　依旧的，需要查看一下是不是配置好了<br>　　<code>git --version</code><br><br> <img src="/2019/12/06/Hexo-install/git_comp.png" alt="Hexo-install"><br>　　配置好之后，从上面失败的地方开始。即使用hexo init继续初始化。<br><br> <img src="/2019/12/06/Hexo-install/hexo_comp.png" alt="Hexo-install"><br>　　然后输入<br>　　<code>hexo s</code><br><br> <img src="/2019/12/06/Hexo-install/hexo_test.png" alt="Hexo-install"><br>进行调试，打开浏览器输入<br>　　<code>localhost:4000</code><br>出现以下界面你说明安装成功<br><br> <img src="/2019/12/06/Hexo-install/hexo_s.png" alt="Hexo-install"><br>　　到了这一步说明你已经安装成功啦，你可以为自己新建一篇命名叫”Hexo_install”博文，命令如下<br>　　<code>hexo n Hexo_install</code><br>这篇文章会生成在你的文件下的source_posts\里，你可以对其进行编辑，注意这里使用的是markdown语言。<br><br> <img src="/2019/12/06/Hexo-install/first_text.png" alt="Hexo-install"><br>　　下一步是把你的博客建在github上面，首先先下载一个插件:<code>cnpm install --save hexo-deployer-git</code>,安装后的结果如图所示。<br><br> <img src="/2019/12/06/Hexo-install/hexo_deployer.png" alt="Hexo-install"><br>　　然后你要在github注册一个账号，申请一个仓库，获取到你的web URL。<br><br> <img src="/2019/12/06/Hexo-install/the_link.png" alt="Hexo-install"><br>　　进入blog文件夹，编辑<code>_config.yml</code>文件，找到deploy一栏，输入以下代码，记得repo一栏填写的是上面在github上获取的那一个序列。<br><br> <img src="/2019/12/06/Hexo-install/config.png" alt="Hexo-install"><br>　　输入完后，在blog目录下输入<code>hexo clean</code>,清除缓存后输入<code>hexo g -d</code>提交，这里会弹出一个窗口让你输入你的github账号和密码。<br><br> <img src="/2019/12/06/Hexo-install/login.png" alt="Hexo-install"><br>　　deploy之后，等待一段时间，输入自己的博客账号，像我的就是<code>账户名.github.io</code>,打开之后即可看自己的博客。</p>
<h2 id="可能会遇到的问题（后面看看会不会更新–）"><a href="#可能会遇到的问题（后面看看会不会更新–）" class="headerlink" title="可能会遇到的问题（后面看看会不会更新–）"></a>可能会遇到的问题（后面看看会不会更新–）</h2><h3 id="怎么配置系统环境"><a href="#怎么配置系统环境" class="headerlink" title="怎么配置系统环境"></a>怎么配置系统环境</h3><h3 id="怎么样在github上创建仓库"><a href="#怎么样在github上创建仓库" class="headerlink" title="怎么样在github上创建仓库"></a>怎么样在github上创建仓库</h3><h3 id="markdown的一些语法"><a href="#markdown的一些语法" class="headerlink" title="markdown的一些语法"></a>markdown的一些语法</h3><p>　　未完待续……</p>
]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
